\documentclass[fleqn,b5paper]{article}

\usepackage{times,mathptm,helvet}
\usepackage{graphicx} % use \usepackage[demo]{graphicx} to suppress figures
\usepackage{pdfsync}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[justification=centering,labelsep=period,figurename=Fig.]{caption}
\usepackage[sort&compress]{natbib}
\usepackage{url}
\usepackage{hyperref}

\hypersetup{colorlinks=true, urlcolor=blue, citecolor=black, linkcolor=black}
\urlstyle{same}

\setlength{\paperwidth}{7.0in}
\setlength{\paperheight}{10.0in}
\setlength{\textwidth}{6.0in}
\setlength{\textheight}{9in}
\setlength{\topmargin}{-.5in}
\setlength{\headheight}{0in}
\setlength{\headsep}{0in}
\setlength{\parindent}{0in}
\setlength{\parskip}{.5\baselineskip}
\setlength{\footskip}{0.25in}
\setlength{\oddsidemargin}{-.5in}
\setlength{\evensidemargin}{-.5in}
\setlength{\mathindent}{0cm}
\setlength{\topsep}{0cm}
\setlength{\labelsep}{.8cm}
\pagestyle{empty}

\frenchspacing % single space between sentences
\widowpenalty=10000
\clubpenalty=10000
\raggedbottom

%% this changed in 2014, but I want to keep around in case they change back
%% define a special equation environment to put eq. numbers at 12.7 cm tab stop
%\newcounter{eqncounter}
%\def\theeqncounter{\arabic{eqncounter}}
%\newenvironment{iafssequation}{\refstepcounter{eqncounter}\begin{sloppypar}\vspace{.5\baselineskip}\noindent\begin{minipage}[s]{12.7cm}$\displaystyle}
%{$\hspace*{\fill}(\theeqncounter)\end{minipage}\vspace{.5\baselineskip}\end{sloppypar}}

\begin{document}

\renewcommand{\thefootnote}{\sf \arabic{footnote}}
\renewcommand{\d}{\mathrm{d}}
\renewcommand{\labelitemi}{{\scriptsize $\bullet$}}
\renewcommand{\labelitemii}{{\scriptsize $\bullet$}}

\newfont{\helvb}{phvb7t at 14pt} % helvetica narrow bold
\newfont{\helvn}{phvr7t at 10pt} % helvetica narrow

\bibliographystyle{unsrtnat}
\setcitestyle{numbers,citesep={,\!}} % cite style is [1,2] instead of [1, 2] (remove space)

\begin{flushleft}
{\helvb Fire Model Validation -- Ten Lessons Learned}\\
\hspace{1in}\\
\helvn KEVIN McGRATTAN\footnotemark[1], RICHARD PEACOCK\footnotemark[1], and KRISTOPHER OVERHOLT\footnotemark[1]  \\
\footnotemark[1] Fire Research Division \\
National Institute of Standards and Technology \\
Gaithersburg, Maryland, USA
\end{flushleft}
\rm

\vspace{\parskip}
{\bf ABSTRACT}

This paper provides a summary of lessons learned by the authors validating fire models.

\vspace{\parskip}
{\bf KEYWORDS:} fire modeling; model validation; model uncertainty

\vspace{\parskip}
{\bf INTRODUCTION}

In 2007, the U.S.~Nuclear Regulatory Commission (NRC) and the Electric Power Research Institute (EPRI) published the results of a validation study of five different fire models commonly used by the commercial nuclear power industry~\cite{NUREG_1824}. The study was prompted by the NRC's adoption in 2004 of the National Fire Protection Association standard NFPA~805, {\em Performance-Based Standard for Fire Protection for Light-Water Reactor Electric Generating Plants}~\cite{NFPA_805}. In particular, NFPA~805 requires fire models to be {\em verified and validated}. The standard does not state explicitly what is meant by this. Guidance documents, like the SFPE {\em Guidelines for Substantiating a Fire Model for a Given Application}~\cite{SFPE_G.06}, and standards documents like ASTM~E~1355, {\em Standard Guide for Evaluating the Predictive Capabilities of Deterministic Fire Models}~\cite{ASTM:E1355}, and ISO~16730, {\em Assessment, Verification and Validation of Calculation Methods,} all provide a basic framework for evaluating models. However, these documents do not have specific requirements as to how the model uncertainty is to be reported and how this information is to be used in a regulatory context. As a result, the NRC and EPRI took it upon themselves to develop a relatively simple framework for reporting and applying model uncertainty in day to day design analyses.

The primary lesson learned from this experience is that most validation studies reported in the literature are not particularly useful for regulatory officials. Some of the reasons for this are (1) journal articles invariably report results of older versions of the model, (2) the proposed application of the model is not always consistent with the application reported in the article, and (3) there is no simple method of applying the reported model uncertainty to the proposed application. Following the publication of the NRC validation study~\cite{NUREG_1824} in 2007, a member of the NRC review panel asked the simple question, ``if I use Model~X to predict Quantity~Y, how do I calculate uncertainty bounds?'' Because the application space of zone and CFD fire models is fairly wide, it is difficult to report the uncertainty of a given calculation in a simple and concise way. Often the regulator is given a list of journal articles attesting to the accuracy of the model, but there remains too much room for interpretation. As a result, the NRC and EPRI published a follow on guidance document, {\em Nuclear Power Plant Fire Modeling Analysis Guidelines}~\cite{NUREG_1934}, in which practical examples demonstrate how to apply the results of the model V\&V study. This paper highlights various aspects of these studies and the important lessons learned.


\vspace{\parskip}
{\bf Lesson 1: Identify the stakeholder}



The primary components of the FDS quality assurance process are the verification and validation~\cite{ASTM:E1355}. In short, verification means checking that the source code contains no errors and corresponds exactly to what is described in the code documentation. Validation, in turn, is the assessment of the code accuracy in its intended use. The results of the validation simulations are used to guide the future development work by revealing the physical quantities and parts of the model having highest uncertainties, and to estimate model uncertainty in a way that is useful for the end users. The validation results are summarized in terms of two uncertainty metrics: systematic bias and the width of the random error distribution for each output quantity.



\vspace{\parskip}
{\bf Lesson 2: Scientific journals are not the best way of reporting results}

Publication of technical results in peer-reviewed, archival journals is the most common form of scientific communication, and over the past thirty years, thousands of papers have been published in the fire literature documenting various forms of fire model V\&V. Standards like ASTM~E~1355 credit journal articles as part of the process of model evaluation:
\begin{quote}
The theoretical basis of the model should be subjected to a peer review $\ldots$ Publication of the theoretical basis of the model in a peer-reviewed journal article may be sufficient to fulfill this review.
\end{quote}
The drawback of journal publications, however, is that once the theoretical basis of the model has been established, the journal articles then become nothing more than a catalog of fairly routine verification exercises and validation studies. Worse, as new versions of the model are released, the journal articles can no longer be cited as the source of the quantified uncertainty results. It is simply impractical to publish again and again the results of validation studies; thus, a better way is needed to assure model users that the currently released version is of comparable or better accuracy than a previously documented version. This better way is for the model developers and interested end users to maintain and regularly update compilations of verification and validation results. This serves two purposes: (1) model users can cite the most recent V\&V results to justify their use of the model, and (2) model developers can be assured that changes to the source code do not adversely affect past results.

As a follow on to the 2007 NRC V\&V study documented in Ref.~\cite{NUREG_1824}, NIST issued validation guides for FDS and CFAST that follow the same general framework. These guides are automatically regenerated with each new release of the software so that the model uncertainty metrics described above are always appropriate for the current version.

\vspace{\parskip}
{\bf Lesson 3: Quantity over quality}

Over the past 40 years, thousands of full-scale fire experiments have been performed at research and testing labs, universities, and consulting companies. Many of these experiments were performed specifically for model validation. In most cases, however, the test reports lack information needed for a complete assessment of the models. For example, experimental uncertainties, material property data, heat release rates, and other boundary conditions are often lacking. It is difficult to know exactly how the measurements were made and the data reduced. If the criteria for data selection is overly strict, there might not be any data left to use. Fortunately, what is lacking in quality can to some extent be made up in quantity. It is unwise to limit the model assessment to one or two sets of data from one laboratory.



\vspace{\parskip}
{\bf Lesson 3: Organize the validation study around predicted quantities}

There is no single metric that can answer the question, ``how accurate is the model?'' The accuracy depends on what is being predicted. For example, the average upper layer temperature in a pre-flashover fire scenario is a relatively easy quantity to predict. Empirical, zone and CFD models have all been shown to predict it nearly to within experimental uncertainty when the heat release rate is specified~\cite{NUREG_1824}. However, quantities such as heat flux, especially to targets near the fire, are more difficult to predict.

The first step of the validation process is the choice of output quantities. In fire safety analysis, typical output quantities are gas or target temperatures, height of the smoke layer within a compartment or the concentration of toxic species. The importance of the heat transfer calculations makes also the flow velocities and heat fluxes interesting quantities for validation. In some cases, the accurate prediction of pressure may be needed. In practical applications, many of the quantities are compared against some critical value monitoring the time to reach this value. Therefore, it may be necessary to validate the capability to predict the time-to-threshold, but usually this is not made because the time-to-threshold has no defined value if the threshold is never met.

\vspace{\parskip}
{\bf Lesson 3: Sophisticated comparison metrics are more trouble than they are worth}

A typical fire model validation study results in hundreds of point to point comparisons of predicted and measured temperatures, heat fluxes, gas concentrations, and so forth, all reported as time histories like the one shown in Fig.~\ref{temp_history}.
\begin{figure}[ht]
\begin{center}
\includegraphics[height=2.5in]{../FDS_Validation_Guide/FIGURES/sample_time_history}
\end{center}
\caption[Sample time history plots.]{Example of a typical time history comparison of model prediction and experimental measurement.}
\label{temp_history}
\end{figure}
Typically, the data is condensed into a more tractable form by way of a single metric with which to compare the two curves like the ones shown in Fig.~\ref{temp_history}. Peacock {\em et al.}~\cite{Peacock:FSJ1999} discuss various possible metrics. A commonly used metric is simply to compare the measured and predicted peak values. This was the choice of the NRC in performing their validation study~\cite{NUREG_1824}, simply because it is a simple, effective way to assess the fire's potential to damage critical equipment. It was also found that when comparing thousands of point to point measurements, the choice of metric did not change the final results.


\vspace{\parskip}
{\bf Do not neglect experimental uncertainty}

Because experimental uncertainty is often not reported, modelers have no choice but to compare their predictions to measurements without knowing the uncertainty of the reported test parameters or the measurements. The {\em experimental uncertainty} is a combination of the uncertainty in the measurement of the quantity of interest, like the gas temperature or heat flux, along with the propagated uncertainty of the measured test parameters, like the heat release rate or material properties.

In the NRC validation study~\cite{NUREG_1824}, Hamins estimated the experimental uncertainty for a variety of full-scale fire experiments. There were two uncertainty estimates needed for each quantity of interest. The first was an estimate, expressed in the form of a 95~\% confidence interval, of the uncertainty in the measurement of the quantity itself. For example, reported gas and surface temperatures were made with thermocouples of various designs (bare-bead, shielded, aspirated) with different size beads, metals, and so on. For each, one can estimate the
uncertainty in the reported measurement. Next, the uncertainty of the measurements of the reported test parameters was estimated, including the heat release rate, leakage area, ventilation rate, material properties, and so on. It was then necessary to calculate how the uncertainty in these parameters contributed to the uncertainty of the reported measurement. To do this, Hamins examined a number of empirical formulae that are widely used in fire protection engineering to determine the most important test parameters and their effect on the measured results. These formulae provided the means of propagating the parameter uncertainties through the experiment. For example, the correlation of McCaffrey, Quintiere and Harkleroad (MQH)~\cite{SFPE:Walton} asserts that the hot gas layer temperature rise, $T-T_0$, due to a compartment fire is proportional to the heat release rate, $\dot{Q}$, raised to the two-thirds power:
\begin{equation}
   T-T_0 = C \, \dot{Q}^{\frac{2}{3}}
\end{equation}
The constant, $C$, involves a number of geometric and thermo-physical parameters that are unique to the given fire scenario. By way of differentials, this empirical relationship can be expressed in the form:
\begin{equation}
   \frac{\Delta T}{T-T_0} \approx \frac{2}{3} \, \frac{\Delta \dot{Q}}{\dot{Q}}
\end{equation}
In words, the relative change in the temperature rise is approximately two-thirds the relative change in the heat release rate. Assuming that the numerical model exhibits the same functional relationship between the compartment temperature and the heat release rate, there is now a way to express the uncertainty of the model prediction as a function of the uncertainty of this most important input parameter. Often, the uncertainty of a measurement is expressed in the form of a 95~\% confidence interval, or two standard deviations or 2$\sigma$.
Thus, if the heat release rate of a fire is assumed with 95~\% confidence to be within 15~\% of the reported measurement, then the temperature predicted by the model has an uncertainty of at least 10~\%. Table~\ref{Parameter_Uncertainty} lists the most important physical parameters associated with various measured quantities in the experiments and their power dependence.

\begin{table}[ht]
\caption{Sensitivity of model outputs from Volume 2 of NUREG-1824~\cite{NUREG_1824}. }
\begin{center}
\begin{tabular}{|l|c|c|}
\hline
Output Quantity                                 & Input Parameter(s)    & Power Dependence \\ \hline \hline
HGL Temperature                                 & HRR                   & 2/3    \\ \hline
HGL Depth                                       & Door Height           & 1      \\ \hline
Gas Concentration                               & HRR                   & 1/2    \\ \hline
                                                & HRR                   & 1      \\ \cline{2-3}
\raisebox{1.5ex}[0pt]{Smoke Concentration}      & Soot Yield            & 1      \\ \hline
                                                & HRR                   & 2      \\ \cline{2-3}
Compartment Pressure                            & Leakage Rate          & 2      \\ \cline{2-3}
                                                & Ventilation Rate      & 2      \\ \hline
Heat Flux                                       & Heat Flux             & 4/3    \\ \hline
Surface Temperature                             & HRR                   & 2/3    \\ \hline
\end{tabular}
\end{center}
\label{Parameter_Uncertainty}
\end{table}

The uncertainty of the measured output quantity and the propagated uncertainty of each significant input parameter are combined via simple quadrature.  Hamins estimated the combined experimental uncertainties for ten quantities of interest in the NRC validation study. The results are summarized in Table~\ref{Uncertainty}, with each combined uncertainty reported in the form of a 95~\% confidence interval (i.e., $2 \, \widetilde{\sigma}_E$).
\begin{table}[ht]
\caption{Summary of Hamins' uncertainty estimates~\cite{NUREG_1824}. }
\begin{center}
\begin{tabular}{|l|c|}
\hline
Measured Quantity               & Combined Relative       \\
                                & Uncertainty, $2 \, \widetilde{\sigma}_E$       \\ \hline \hline
HGL Temperature                 & 0.14    \\ \hline
HGL Depth                       & 0.13    \\ \hline
Ceiling Jet Temperature         & 0.16    \\ \hline
Plume Temperature               & 0.14    \\ \hline
Gas Concentrations              & 0.09    \\ \hline
Smoke Concentration             & 0.33    \\ \hline
Pressure with Ventilation       & 0.80    \\ \hline
Pressure without Ventilation    & 0.40    \\ \hline
Heat Flux                       & 0.20    \\ \hline
Surface Temperature             & 0.14    \\ \hline
\end{tabular}
\end{center}
\label{Uncertainty}
\end{table}



\vspace{\parskip}
{\bf Lesson 3: Develop a simple way to report model uncertainty}

Perhaps the simplest expression of model uncertainty is to report that Model~X over or under-predicts Quantity~Y by Z~\%. In many instances this simple statement might be all that is required to demonstrate that a given model is appropriate for a given application. The relative uncertainty, Z, can be calculated simply by averaging the relative differences of the predicted and measured values. If there are an abundance of measurements from a variety of experiments, and these experiments are comparable in scope to the proposed application of the model (in the sense to be discussed below), then this simple quantification of model uncertainty may be sufficient to determine if the model is appropriate. However, with a bit more work, this simple quantification of model uncertainty can be expanded to include a measure of the scatter. It is not unusual, especially when there are many measurement points, that the {\em average} relative difference is quite small, often within a few percent. However, the scatter about the average can be considerable. In addition to the average relative difference, this scatter, expressed in the form of a standard deviation, can be used as uncertainty bounds for the predictions. This is particularly important to a regulatory agency like the NRC which routinely performs probabilistic risk assessments (PRA).

The method for calculating model uncertainty is described below. Further details on the method can be found in Ref.~\cite{McGrattan:Metrologia}. The goal of this analysis is to quantify the performance of the model in predicting a given quantity of interest (e.g., the hot gas layer temperature) with just two parameters; one that expresses the tendency for the model to under or over-predict the true value and one that expresses the degree of scatter about the true value.

Regardless of the exact form of the metric, what results from this exercise is a pair of numbers for each time history, $(E_i,M_i)$, where $i$ ranges from 1 to $n$ and both $M_i$ and $E_i$ are positive numbers expressing the increase in the value of a quantity above its ambient. As mentioned above, measurements from full-scale fire experiments often lack uncertainty estimates. In cases where the uncertainty is reported, it is usually expressed as either a standard deviation or confidence interval about the measured value. In other words, there is rarely a reported systematic bias in the measurement because if a bias can be quantified, the reported values are adjusted accordingly. For this reason, assume that a given experimental measurement, $E_i$, is normally distributed about the ``true'' value, $\theta_i$, and there is no systematic bias:
\begin{equation}
   E \; | \; \theta \sim N(\theta \; , \; \sigma_E^2) \label{expunc}
\end{equation}
The notation $E \; | \; \theta$ means that $E$ is conditional on a particular value of $\theta$. This is the usual way of defining a likelihood function. An important assumption in this analysis is that the model and experimental uncertainty can be expressed in {\em relative} terms. This assumption is based on the observation that the results of past validation exercises, when plotted as shown in Fig.~\ref{Normality}, form a wedge-shaped pattern that suggests that the difference between predicted and measured values is roughly proportional to the magnitude of the measured value. A common way in statistics of dealing with relative uncertainties is to work with the natural logarithm of the various quantities. To understand why, consider the so-called delta method in statistics~\cite{Oehlert:1992} where the distribution of a function of a normally distributed random variable $X \sim N(\mu,\sigma^2)$ is estimated:
\[g(X) \sim N \left( g(\mu) + g''(\mu) \, \sigma^2/2 \, , \, (g'(\mu) \, \sigma)^2\right)\]
The delta method provides an approximate distribution of the natural log of the random variable $E$:
\begin{equation}
   \ln E \; | \; \theta \sim N \left( \ln \theta - \frac{\widetilde{\sigma}_E^2}{2} \, , \,\widetilde{\sigma}_E^2 \right) \label{eeq}
\end{equation}
The purpose of applying the natural log to the random variable is so that its variance can be expressed in terms of the relative uncertainty, $\widetilde{\sigma}_E=\sigma_E/\theta$.

It cannot be assumed, as in the case of the experimental measurements, that the model predictions have no systematic bias. Instead, it is assumed that the model predictions are normally distributed about the true values multiplied by a bias factor, $\delta$:
\begin{equation}
   M \; | \; \theta \sim N \left(\delta \, \theta \, , \, \sigma_M^2 \right) \label{mdist}
\end{equation}
The standard deviation, $\sigma_M$, and the bias factor, $\delta$, characterize the model uncertainty. Again, the delta method renders a distribution for $\ln M$ whose parameters can be expressed in terms of a relative standard deviation:
\begin{equation}
   \ln M \; | \; \theta \sim N \left(\ln \delta +\ln \theta - \frac{\widetilde{\sigma}_M^2}{2} \; , \;
   \widetilde{\sigma}_M^2 \right) \quad ; \quad \widetilde{\sigma}_M=\frac{\sigma_M}{\delta \, \theta} \label{meq}
\end{equation}
Combining Eq.~(\ref{eeq}) with Eq.~(\ref{meq}) yields:
\begin{equation}
   \ln M  - \ln E = \ln(M/E) \sim N \left( \ln \delta - \frac{\widetilde{\sigma}_M^2}{2}+\frac{\widetilde{\sigma}_E^2}{2} \; ,
   \; \widetilde{\sigma}_M^2+\widetilde{\sigma}_E^2 \right) \label{lnMeq}
\end{equation}
Equation~(\ref{lnMeq}) is important because it does not involve the unknown $\theta$, but rather the known predicted and measured values of the quantity of interest plus an estimate of the experimental uncertainty. The mean of this distribution is estimated:
\begin{equation}
   \overline{\ln (M/E)} \approx \frac{1}{n} \, \sum_{i=1}^n \, \ln (M_i/E_i)
\end{equation}
The variance is estimated:
\begin{equation}
   \mathrm{Var} \Big( \ln (M/E) \Big) \approx \frac{1}{n-1} \sum_{i=1}^n \, \left[ \ln (M_i/E_i) - \overline{\ln (M/E)}  \right]^2 \label{stdev}
\end{equation}
from which the relative model standard deviation can be estimated:
\begin{equation}
   \widetilde{\sigma}_M \approx \sqrt{ \mathrm{Var}\Big( \ln (M/E) \Big) - \widetilde{\sigma}_E^2 } \label{sig_M}
\end{equation}
and the bias factor:
\begin{equation}
   \delta \approx \exp \left( \overline{\ln (M/E)} + \frac{ \widetilde{\sigma}_M^2}{2}-\frac{\widetilde{\sigma}_E^2}{2} \right) \label{delta}
\end{equation}
Equation~(\ref{sig_M}) imposes a constraint on the value of the experimental uncertainty, $\widetilde{\sigma}_E$. A further constraint is that $\widetilde{\sigma}_M$ cannot be less than $\widetilde{\sigma}_E$ because it is not possible to demonstrate that the model is more accurate than the measurements against which it is compared. Combining the two constraints leads to:
\begin{equation}
   \widetilde{\sigma}_E^2 < \frac{1}{2} \mathrm{Var}\Big( \ln (M/E) \Big)
\end{equation}
Equation~(\ref{mdist}) states that the model prediction, $M$, is normally distributed about the true value times the bias factor, $\delta \theta$. Under certain conditions~\cite{McGrattan:Metrologia}, it can be assumed that $\delta \theta$ is likewise distributed about $M$:
\begin{equation}
   \delta \theta \; | \; M \sim N \left(M \, , \, \sigma_M^2 \right) \label{thetaeq}
\end{equation}
Dividing by the bias factor, $\delta$, and applying the delta method once again yields:
\begin{equation}
   \theta \; | \; M \sim N \left( \frac{M}{\delta} \; , \; \widetilde{\sigma}_M^2 \left( \frac{M}{\delta} \right)^2 \right) \label{truth}
\end{equation}
In words, this expression states that the true value of a given quantity is normally distributed about the predicted value. The mean and standard deviation of the distribution are functions of the measured and predicted values of experiments that are comparable in scope to the scenario of interest. Before demonstrating how to use Eq.~(\ref{truth}), an aside. Until now, it has been assumed that all of the relevant uncertainties can be characterized by normal distributions. Equation~(\ref{lnMeq}) can be used to test this assumption, whereas Eqs.~(\ref{mdist}) and (\ref{expunc}) cannot. An examination of the results of the validation study discussed above~\cite{NUREG_1824} indicates that in cases where the number of pairs of $M$ and $E$ is greater than about 20, the values of $\ln (M/E)$ pass a common test for normality. Figure~\ref{Normality} contains a set of data from the validation study. There are 124 data points, for which $\ln (M/E)$ conforms quite well to a normal distribution. If normality cannot be assumed, alternative techniques for assessing model uncertainty with limited experimental data have been proposed, for example, see Ref.~\cite{Siu:1992}.
\begin{figure}[ht!]
\begin{tabular}{ll}
\includegraphics[width=2.8in]{../FDS_Validation_Guide/FIGURES/Wall_Temperature_Scatter} &
\includegraphics[width=2.8in]{../FDS_Validation_Guide/FIGURES/Wall_Temperature_Normality}
\end{tabular}
\caption[Testing the normality of validation data.]{The results of normality testing for a set of data taken from Reference~\cite{NUREG_1824}. On the
left is a comparison of measured vs predicted wall temperatures, and on the right is the distribution of $\ln(M/E)$. Note that the off-diagonal lines in the scatter plot
indicate the $2 \widetilde{\sigma}$ bounds for the experiments (long dash) and the model (short dash).}
\label{Normality}
\end{figure}

\begin{figure}[h!]
\centering
\includegraphics[width=.5\linewidth]{../FDS_Validation_Guide/SCRIPT_FIGURES/ScatterPlots/Wall_Temperature}
\vskip-.2cm
\caption{Comparison of measured and predicted wall temperatures.}
\label{wall_temp}
\end{figure}

The results of the analysis described above are reported in the form of scatter plots similar to the one shown in Fig.~\ref{wall_temp}. It compares the measured and predicted wall surface temperatures for a particular model for several different sets of experiments indicated by the different symbols. The experimental and model uncertainties as well as the bias factor are reported in the upper left corner. The standard deviations are reported in the form of 95~\% confidence intervals, or $2\widetilde{\sigma}$. The dashed off-diagonal lines depict these values graphically. The longer-dashed off-diagonal lines, which are centered about the solid diagonal line, indicate the experimental uncertainty.  The solid off-diagonal line just above the diagonal line represents the model bias. For example, the bias factor for the data shown in Figure~\ref{wall_temp} is 1.02. This means that the model has been shown to slightly over-estimate wall surface temperatures by 2~\%, on average. The second set of off-diagonal lines, shown as short-dashed lines, indicates the model uncertainty.

The information in Fig.~\ref{wall_temp} can be used as follows. Suppose this fire model is used to estimate the likelihood that electrical control cables could be damaged due to a fire in a compartment. Damage is assumed to occur when the surface temperature of any cable reaches 200~$^\circ$C above ambient. The probability that the cables would be damaged if the model predicts that the maximum surface temperature rise is 175~$^\circ$C is given by the {\em complimentary error function}:
\begin{equation}
   P(T>T_c) = \frac{1}{2} \hbox{erfc} \left( \frac{T_c - \mu}{\sigma \sqrt{2}} \right) \quad ; \quad \mu=175/\delta \quad ; \quad \sigma = \widetilde{\sigma}_M \, \frac{175}{\delta}
\end{equation}
This means that there is a 6~\% chance that the cables could become damaged, assuming that the model's input parameters are not
subject to uncertainty.


\vspace{\parskip}
{\bf Lesson 4: Clearly state for what range of parameters the study is applicable}

The use of fire models to support fire protection decision making requires a good understanding of their limitations and predictive capabilities. NFPA 805 (NFPA, 2001) states that fire models shall only be applied within the limitations of the given model and shall be verified and validated. To support risk-informed/performance-based fire protection and implementation of the voluntary rule that adopts NFPA 805 as an RI/PB alternative, the NRC RES and EPRI conducted a collaborative project for the V\&V of the five selected fire models. The results of this project were documented in NUREG-1824 (EPRI 1011999), Verification and Validation of Selected Fire Models for Nuclear Power Plant Applications.
Twenty-six full-scale fire experiments from six different test series were used to evaluate the models' ability to estimate thirteen quantities of interest for fire scenarios that were judged to be typical of those that might occur in a nuclear power plant. Each series represented a typical fire scenario (for example, a fire in a switchgear room or turbine hall); however, the test parameters could not encompass every possible NPP fire scenario. Five fire models were selected for the study, based on the fact that they are commonly used in fire analyses of nuclear plants in the U.S. Two of the models consist of simplified engineering correlations, two are ``zone'' models, and one is a CFD model (FDS).
To better understand the range of applicability of the validation study, Table 2 lists various normalized parameters that may be used to compare NPP fire scenarios with the validation experiments. These parameters express, for instance, the size of the fire relative to the size of the room, or the relative distance from the fire to critical equipment. This information is important because typical fire models are not designed for fires that are very small or very large in relation to the volume of the compartment or the ceiling height. For a given set of experiments and NPP fire scenarios, the user can calculate the relevant normalized parameters. These parameters will either be inside, outside, or on the margin of the validation parameter space. Consider each case in turn:

If the parameters fall within the ranges that were evaluated in the validation study, then the results of the study can be referenced directly.

If only some of the parameters fall within the range of the study, additional justification is necessary. This is a common occurrence because realistic fire scenarios involve a variety of fire phenomena, some of which are easier to estimate than others. A case in point is the burning of electrical cabinets and cables. NUREG-1824 (EPRI 1011999) does not address these fires directly, even though some of the experiments used in the study were intended as mock-ups of control or switchgear room fires. For scenarios involving these kinds of fires, the heat release rates are often taken from experiments rather than predicted by a model. It has been shown, in NUREG-1824 (EPRI 1011999) and other validation studies, that the models can estimate the transport of smoke and heat with varying degrees of accuracy, but they have not been shown (at least not in NUREG-1824 (EPRI 1011999)) to estimate the details of the fire's ignition and growth. While this does not eliminate the models from the analysis, it still restricts their applicability to only some of the phenomena.

If the parameters fall outside the range of the study, then a validation determination cannot be made based on the results from the study. The modeler needs to provide independent justification for using the particular model. For example, none of the experiments considered in NUREG-1824 (EPRI 1011999) were under-ventilated. However, several of the models have been independently compared to under-ventilated test data, and the results have been documented either in the literature or in the model documentation. As another example, suppose that the selected model uses a plume, ceiling jet, or flame height correlation outside the parameter space of NUREG-1824 (EPRI 1011999) but still within the parameter space for which the correlation was originally developed. In such cases, appropriate references are needed to demonstrate that the correlation is still appropriate even if not explicitly validated in NUREG-1824 (EPRI 1011999).

This ``parameter space'' outlines the range of applicability of the validation studies performed to date. The parameters are explained below:

The \underline{Fire Froude Number}, $\dot{Q}^*$, is a useful non-dimensional quantity for plume correlations and flame height estimates:
\begin{equation} Q^* = \frac{\dot{Q}}{\rho_\infty c_p T_\infty \sqrt{gD} D^2} \end{equation}
Here, $D$, is the equivalent diameter of the base of the fire, calculated $D=\sqrt{4A/\pi}$, where $A$ is the area of the base, $\dot{Q}$ is the peak heat release rate, $\rho_\infty$, $c_p$, $T_\infty$, and $g$ are the ambient gas density, specific heat, temperature and acceleration of gravity. The Fire Froude Number is essentially the ratio of the fuel gas exit velocity and the buoyancy-induced plume velocity. Jet fires are characterized by large Froude numbers. Typical accidental fires have a Froude number near unity.

The \underline{Flame Height relative to Ceiling Height}, $L_f/H$, is a convenient way to express the physical size of the fire relative to the size of the room. The height of the visible flame, based on Heskestad's correlation, is estimated by:
\begin{equation} L_f = D \, \left( 3.7 \, (Q^*)^{2/5} - 1.02 \right) \end{equation}

The \underline{Global Equivalence Ratio}, $\phi$, is the ratio of the mass flux of fuel to the mass flux of oxygen into the compartment, divided by the stoichiometric ratio.
\begin{equation} \phi = \frac{\dot{m}_f}{r\, \dot{m}{\hbox{\tiny O$_2$}}} \equiv  \frac{\dot{Q} \, \hbox{(kW)}}{13,100 \, \hbox{(kJ/kg)} \; \dot{m}_{\hbox{\tiny O$_2$}} } \quad ; \quad  \dot{m}_{\hbox{\tiny O$_2$}} = \left\{
   \begin{array}{r@{\quad:\quad}l}
      \frac{1}{2} \, 0.23 \, A_0 \sqrt{H_0} & \hbox{Natural Ventilation} \\[0.1in]
      0.23 \, \rho \, \dot{V}       & \hbox{Mechanical Ventilation} \end{array} \right.
\end{equation}
Here, $r$ is the stoichiometric ratio, $A_0$ is the area of the compartment opening, $H_0$ is the height of the opening, $\rho$ is the density of air, and $\dot{V}$ is the volume flow of air into the compartment. If $\phi<1$, the compartment is considered ``well-ventilated'' and if $\phi>1$, the compartment is considered ``under-ventilated.''

The \underline{Compartment Aspect Ratios}, $W/H$ and $L/H$, indicate if the compartment is shaped like a hallway, typical room, or vertical shaft. This parameter is of particular importance for zone models because a number of empirical correlations used by zone models are not applicable for narrow hallways or shafts.

The \underline{Relative Distance along the Ceiling}, $r_{cj}/H$, indicates the distance from the fire plume of a sprinkler, smoke detector, etc., relative to the compartment height, $H$. This parameter is useful whenever the ceiling jet is an important component of the analysis.

The \underline{Relative Distance from the Fire}, $r_{rad}/D$, indicates whether a ``target'' is near or far from the fire. This parameter is useful when evaluating radiation calculation ranging from the simple point source method all the way through sophisticated solutions of the radiation transport equation.

\bibliography{../Bibliography/FDS_general}




\end{document}
