
\newcommand{\paper}{chapter }
%\newcommand{\paper}{paper }

\chapter{Quantifying Model Uncertainty}

\section{Introduction}

Although there are numerous definitions of uncertainty in regard to fire modeling, the most clearly understood is
probably by way of a simple example.
Suppose a fire model is used to estimate the likelihood that electrical control cables would be damaged due to
a fire.  It is assumed that the fire is ignited within an electrical cabinet, and that
damage to exterior cables is assumed to occur when the surface temperature of any cable reaches 400~$^\circ$C.
The model predicts that the maximum surface temperature of the cables would be 350~$^\circ$C.
Does this mean that there is no chance of damage? The answer is no, because the input parameters, like the heat release rate of
the cabinet fire, and the model assumptions, like the way the fire and the cables are modeled, are uncertain. The effect of the
two combined -- the parameter uncertainty and the model uncertainty -- represent the total uncertainty in the
predicted temperature.

Uncertainty analysis has long been a part of fire science. Indeed, fire modeling has its origins in risk assessment and uncertainty analysis. The scenario above,
for example, was considered by Sui and Apostolakis in the early 1980s~\cite{Sui:RE1982} as part of their development of risk models for
nuclear power plants. The fire models at the
time were relatively simple, which made the methods for quantifying their uncertainty reasonably tractable for practicing engineers. Over the past
thirty years, however, both fire modeling and the corresponding methods of uncertainty analysis have become far more complex. While both are still
areas of active research, it can be argued that while fire modeling has become commonplace among practicing fire protection engineers, uncertainty
analysis has not. There are a number of reasons for this, including:
\begin{enumerate}
\item It is time-consuming,
\item It is difficult to understand, and
\item It is typically not demanded by authorities having jurisdiction (AHJ).
\end{enumerate}
Whether or not these are valid does not change the fact that detailed uncertainty analyses are rarely performed in practice. At best, model developers
and users publish validation studies whose conclusions are expressed in ways like this: ``The model generally over-predicts the measured temperatures
by about 10~\%,'' or ``The model predictions are within about 20~\% of the measured heat fluxes.'' This information helps to inform the decision of
the Authority Having Jurisdiction (AHJ), but only in the most qualitative sense. In particular, if the model has been shown to over-predict the quantity of interest, it is considered
``conservative,'' and its subsequent predictions are assessed in that light.

As performance based design increases the use of fire modeling, and the results of the model are incorporated within a statistical framework,
like a probabilistic risk assessment (PRA), there is a need for a more formal approach to uncertainty that is still tractable for the practicing
engineer. Using the example above, the answer to the question as to whether or not cables would be damaged in a fire could be expressed in terms of
a statistical distribution like the graph shown in Fig.~\ref{bell_curve}. The area underneath the curve is 1, and
the area indicated by the shaded region is the probability that even though the model has predicted 350~$^\circ$C, there is still a chance that
the cables could be damaged because of the uncertainty in the model and its input parameters.
\begin{figure}[t]
\begin{center}
\includegraphics[width=5.in]{FIGURES/bell_curve}
\end{center}
\caption[Demonstration of model uncertainty.]{Plot showing a possible way of expressing the uncertainty of the model prediction.}
\label{bell_curve}
\end{figure}

The uncertainty in the prediction of a fire model results from the model's input parameters and
the model's physical and mathematical assumptions. The two are commonly referred to as
the {\em parameter uncertainty} and the {\em model uncertainty}, respectively. In general, there are numerous input
parameters and numerous subroutines that are part of any fire model calculation. Uncertainty analysis seeks to
assess the impact of each on the final prediction, a considerable problem given all the variables involved.
Fire protection engineering has two advantages that make this problem more tractable. First,
a number of useful empirical correlations have
been developed over the past few decades that provide simple mathematical relationships between the most important input parameters
and output quantities that are of interest in a fire analysis. Second, there is a considerable amount of
experimental measurements against which to compare the results of numerical simulations.

This \paper suggests a methodology for quantifying the {\em model uncertainty}; that is, the accuracy of the model, not its
input parameters. The procedure is not a dramatic departure from the current
practice of fire model validation in that it relies mainly on comparisons of model predictions and experimental measurements. In
assessing the accuracy of the measurements, however, there is a need to quantify how the measured parameters in the experiments
affect the reported results. This suggests a method of handling {\em parameter uncertainty} for the model as well.



\subsection{Sources of Model Uncertainty}

A deterministic fire model is based on fundamental conservation laws of mass, momentum and energy,
applied either to entire compartments or smaller control
volumes that make up the compartments. A CFD model may use millions of control volumes to compute the
solution of the Navier-Stokes equations.
However, it does not actually solve the Navier-Stokes equations, but rather an approximate form of these equations.
The approximation involves simplifying
physical assumptions, like the various techniques for treating subgrid-scale turbulence.
One critical approximation is the discretization of the governing equations. For example,
the partial derivative of the density, $\rho$,
with respect to the spatial coordinate, $x$, can be written in approximate form as:
\be \frac{\partial \rho}{\partial x} = \frac{\rho_{i+1} - \rho_{i-1}}{2 \, \dx} + \mathcal{O}(\dx^2) \ee
where $\dx$ is the grid spacing chosen by the model user.
The second term on the right represents all of the terms of order $\dx^2$ and higher in the Taylor
series expansion and are known collectively as the
{\em discretization error}. These extra terms are simply dropped from
the equation set, the argument being that they become smaller and smaller with decreasing grid cell size, $\dx$.
The effect of these neglected terms is captured, to
some extent, by the subgrid-scale turbulence model, but that is yet another approximation of the true physics.
What effect do these approximations have on
the predicted results? It is very difficult to determine based on an analysis of the discretized equations.
One possibility for estimating
the magnitude of the discretization error is to perform a detailed
convergence analysis, but this still does not answer a
question like, ``What is the uncertainty of the model prediction of the gas
temperature at a particular location in the room at a particular point in time?''

To make matters worse, there are literally dozens of subroutines that make up a CFD fire model,
from its transport equations, radiation solver, solid phase heat transfer routines, pyrolysis model,
empirical mass, momentum and energy transfer routines at the wall, and so on.
It has been suggested by some that
a means to quantify the model uncertainty is to combine the uncertainties of all the model
components in a way similar to that of the input parameters.
However, such an exercise is very difficult, especially for a computational fluid dynamics (CFD) model,
for a number of reasons. First, fire involves
a complicated interaction of gas and solid phase phenomena that are closely coupled.
Second, grid sensitivity in a CFD model or the error associated with
a two-layer assumption in a zone model are dependent on the particular fire scenario.
Third, fire is an inherently transient phenomenon in which relatively small
changes in events, like a door opening or sprinkler actuation, can lead to significant changes in outcome.

Rather than attempt to decompose the model into its constituent parts and assess the uncertainty of
each, the strategy adopted here is to compare model predictions to as many
experiments as possible. This has been the traditional approach for quantifying model uncertainty in fire
protection engineering because of the relative abundance of test data.
However, before the model uncertainty can be quantified, the uncertainty of the
measurements against which the model predictions are compared must be quantified. This is discussed in the next section.


\subsection{Experimental Uncertainty}

In a recent fire model validation study conducted by the U.S.~Nuclear Regulatory Commission~\cite{NUREG_1824},
Hamins estimated the experimental uncertainty for several full-scale fire experiments. There were
two uncertainty estimates needed for each quantity of interest. The first was an estimate, expressed in the
form of a 95~\% confidence interval, of the
uncertainty in the measurement of the quantity itself. For example, reported gas and surface temperatures
were made with thermocouples of various designs (bare-bead,
shielded, aspirated) with different size beads, metals, and so on. For each, one can estimate the
uncertainty in the reported measurement.

Next, the
uncertainty of the measurements of the reported test parameters was estimated, including the heat release rate,
leakage area, ventilation rate, material
properties, and so on. It was then necessary to calculate how the uncertainty in these parameters contributed to
the uncertainty of the reported measurement. To do this, Hamins
examined a number of empirical formulae that are widely used in fire protection engineering to determine the
most important test parameters and their effect on the measured results. These formulae provided the means of propagating
the parameter uncertainties through the experiment.
For example, it has been shown~\cite{SFPE:Walton} that the hot gas layer temperature rise, $T-T_0$, due to
a compartment fire is proportional to the heat release rate, $\dQ$, raised to the two-thirds power:
\be T-T_0 = C \, \dQ^{\frac{2}{3}} \ee
The constant, $C$, involves a number of geometric and thermo-physical parameters that are unique to the given
fire scenario. By way of differentials, this empirical relationship can be expressed in the form:
\be \frac{\Delta T}{T-T_0} \approx \frac{2}{3} \, \frac{\Delta \dQ}{\dQ}  \ee
In words, the relative change in the temperature rise is approximately two-thirds the relative change
in the heat release rate. Assuming that the numerical model exhibits the same functional relationship between the compartment
temperature and the heat release rate, there is now a way to express the uncertainty of the model prediction as a function
of the uncertainty of this most
important input parameter. Often, the uncertainty of a measurement is expressed in the form of a 95~\% confidence interval,
(two standard deviations or 2$\sigma$).
Thus, if the heat release rate of a fire is assumed with 95~\% confidence to be
within 15~\% of the reported measurement, then the temperature
predicted by the model has an uncertainty\footnote{An uncertainty that is expressed in the form of a
percentage typically means one or two {\em relative} standard deviations, often denoted with a tilde,
$\widetilde{\sigma}=\sigma/\mu$.} of at least 10~\%.

Table~\ref{Parameter_Uncertainty} lists the most important physical parameters associated with various
measured quantities in the experiments and their power dependence.


\begin{table}[t]
\caption{Sensitivity of model outputs from Volume 2 of NUREG-1824~\cite{NUREG_1824}. }
\begin{center}
\begin{tabular}{|l|c|c|}
\hline
Output Quantity                                 & Input Parameter(s)    & Power Dependence \\ \hline \hline
HGL Temperature                                 & HRR                   & 2/3    \\ \hline
HGL Depth                                       & Door Height           & 1      \\ \hline
Ceiling Jet Temperature                         & HRR                   &        \\ \hline
Plume Temperature                               & HRR                   &        \\ \hline
Gas Concentration                               & HRR                   & 1/2    \\ \hline
                                                & HRR                   & 1      \\ \cline{2-3}
\raisebox{1.5ex}[0pt]{Smoke Concentration}      & Soot Yield            & 1      \\ \hline
                                                & HRR                   & 2      \\ \cline{2-3}
Compartment Pressure                            & Leakage Rate          & 2      \\ \cline{2-3}
                                                & Ventilation Rate      & 2      \\ \hline
Heat Flux                                       & Heat Flux             & 4/3    \\ \hline
Surface Temperature                             & HRR                   & 2/3    \\ \hline
\end{tabular}
\end{center}
\label{Parameter_Uncertainty}
\end{table}


It was assumed that the measurement and parameter uncertainty are uncorrelated, and they were combined by
quadrature (summing of squares) to yield a combined
experimental uncertainty. Another way to look at this is to recognize that the combined uncertainty is
represented as the diagonal of the rectangle formed
from the horizontal and vertical uncertainty/error bars in Fig.~\ref{scatterplot}.

Hamins performed this exercise for ten quantities of interest in the U.S. NRC validation study.
The results are summarized in Table~\ref{Uncertainty}, with
each combined uncertainty reported in the form of a 95~\% confidence interval ({\em i.e.} $2 \, \widetilde{\sigma}_E$).
The tilde above the $\sigma$ denotes a
{\em relative uncertainty}, which is a convenient way to report it because it is assumed that the uncertainty in the
reported value is proportional to
its magnitude. This assumption is made throughout the analysis for both measurement uncertainty and model error.
The assumption is based on a
qualitative assessment of dozens of scatter plots similar to that shown in Fig.~\ref{scatterplot} that show
the scattered points to form an expanding ``wedge''
about the diagonal line, or some other off-diagonal line due to an assumed bias in the model predictions.
This assessment is a critical component of the
analysis described in the next section.

\begin{table}[t]
\caption{Summary of Hamins' uncertainty estimates~\cite{NUREG_1824}. }
\begin{center}
\begin{tabular}{|l|c|}
\hline
Measured Quantity               & Combined Relative       \\
                                & Uncertainty, $2 \, \widetilde{\sigma}_E$       \\ \hline \hline
HGL Temperature                 & 0.14    \\ \hline
HGL Depth                       & 0.13    \\ \hline
Ceiling Jet Temperature         & 0.16    \\ \hline
Plume Temperature               & 0.14    \\ \hline
Gas Concentrations              & 0.09    \\ \hline
Smoke Concentration             & 0.33    \\ \hline
Pressure with Ventilation       & 0.80    \\ \hline
Pressure without Ventilation    & 0.40    \\ \hline
Heat Flux                       & 0.20    \\ \hline
Surface Temperature             & 0.14    \\ \hline
\end{tabular}
\end{center}
\label{Uncertainty}
\end{table}






\section{Calculating Model Uncertainty}

This section describes a method for calculating the {\em model uncertainty}. In terms of the example given above, this
is the answer to the question, ``350~$^\circ$C plus or minus what?'' The focus is on the model, not the parameter, uncertainty;
thus, it is assumed, for the moment, that the input parameters are not subject to any uncertainty.
In that case, how good is the prediction, assuming the fire scenario is exactly defined?
The answer to this question is based solely on
the model's track record for predicting cable temperatures in controlled validation experiments.

A fire model validation study typically consists of comparing point measurements from a wide variety of fire experiments
with corresponding model predictions.
Figure~\ref{temp_history} is a typical result for a single point measurement, and given that usually
dozens of such measurements are made during each experiment,
and potentially dozens of experiments are conducted as part of a test series, hundreds of such plots can be
produced for any given quantity of interest.
\begin{figure}[t]
\begin{center}
\includegraphics[height=2.5in]{FIGURES/sample_time_history}
\end{center}
\caption[Sample time history plots.]{Example of a typical time history comparison of model prediction and experimental measurement.}
\label{temp_history}
\end{figure}
Usually, the data is condensed into a more tractable form by way of a single metric with which to
compare the two curves like the ones shown in Fig.~\ref{temp_history}. Peacock {\em et al.}~\cite{Peacock:FSJ1999}
discuss various possible metrics. A commonly used metric is simply to compare the measured and predicted peak values.
If the data is spiky, some form of time-averaging can be used. Regardless of the exact form of the metric, what results from
this exercise is a pair of numbers for each plot, $(E_i,M_i)$, that can be depicted graphically as shown in Fig.~\ref{scatterplot}.
The diagonal line in the plot indicates where a prediction and measurement agree.
But because there is uncertainty associated with each, it cannot be said that the model is perfect if its predictions
agree exactly with measurements.
There needs to be a way of quantifying the uncertainties of each before any conclusions can be drawn.
Such an exercise would result in the uncertainty
bars\footnote{The data in Fig.~\ref{scatterplot} was extracted from Ref.~\cite{NUREG_1824}.
The uncertainty bars are for demonstration only.}
shown in the figure. The
horizontal bar associated with each point represents the uncertainty in the measurement itself.
For example, a thermocouple used to measure a gas
temperature has uncertainty. The vertical bar, however, results from the combination of the model
and parameter uncertainty, like the heat release rate and material properties that are reported by the experimentalist.
The {\em experimental uncertainty} is the combination of the measurement and input parameter uncertainty.
Decoupling the model from the experimental uncertainty is the subject of the analysis below.

\begin{figure}[t]
\begin{center}
\includegraphics[height=3.in]{FIGURES/scatterplot}
\end{center}
\caption[Sample scatter plot.]{Example of a typical scatter plot of model predictions and experimental measurements.}
\label{scatterplot}
\end{figure}


Actually, the ``plus or minus'' uncertainty bounds are expressed as a statistical distribution. In other words, the
prediction is not expressed merely as 350~$^\circ$C, but rather as a random variable with a given mean, $\mu$, and standard
deviation, $\sigma$. There are a few very important assumptions to make at the start:
\begin{enumerate}
\item The distribution is assumed to be normal.
\item The degree to which the model under or over-predicts the ``truth'' is expressed as a multiplicative bias factor, $\delta$; thus,
the mean of the distribution is $M/\delta$, where $M$ is the model prediction.
\item The standard deviation of the distribution is expressed as a fixed percentage of the mean,
$\widetilde{\sigma}=\sigma/\mu$.
\end{enumerate}
Under these assumptions, the ``true'' temperature of the cable, $\theta$, is expressed in the
following way\footnote{$N(\mu,\sigma^2)$ denotes a normal (Gaussian) distribution
with mean, $\mu$, and standard deviation, $\sigma$.}:
\be
   \theta \; | \; 350 \sim N \left( \frac{350}{\delta} \; , \; \widetilde{\sigma}_M^2
   \left( \frac{350}{\delta} \right)^2 \right) \label{truthexample}
\ee
In words, given a model prediction of 350~$^\circ$C, the expected value of the true temperature is assumed to be normally distributed
with the given mean and standard deviation. The values of $\delta$ and $\widetilde{\sigma}_M$ are calculated based on the
comparison of model predictions with experimental measurements. This calculation is discussed below. First, however, the
assumptions above require justification.

The first assumption is based on the observation that the results of past validation exercises,
when plotted as shown in Fig.~\ref{scatterplot}, suggest
that the difference between predicted and measured values is roughly proportional to the magnitude of the measured value.
Furthermore, given the
complexity of the models, it would be difficult to postulate a more precise functional relationship.
The same is true of the form of the distribution. Given the complexity of the models and the experiments,
it would be difficult to justify
any particular distribution. In fact, the very reason for developing this method of quantifying model
error based on validation
experiments is because the model algorithm itself is too complicated to work with directly
as a means of estimating the error. However, if
the distributions were better characterized, the methodology presented here could be generalized appropriately.

With these assumptions in mind, a relatively simple method of analyzing the data can be developed.
A general discussion of Bayesian data analysis can be found in
Ref.~\cite{Gelman:Stats}.
Assume that the set of model predictions and the corresponding set of experimental measurements are denoted
$M_i$ and $E_i$, respectively, where $i$ ranges from 1 to $n$ and both $M_i$ and $E_i$ are positive numbers
expressing the increase in the value of a quantity above its ambient.
As mentioned above, measurements from full-scale fire experiments often lack uncertainty estimates. In cases where the uncertainty is
reported, it is usually expressed as either a standard deviation or confidence interval about the measured value. In other words, there is rarely
a reported systematic bias in the measurement because if a bias can be quantified, the reported values are adjusted accordingly.
For this reason, assume that a given experimental measurement, $E_i$, is normally
distributed about the ``true'' value, $\theta_i$, and there is no systematic bias:
\be
   E \; | \; \theta \sim N(\theta \; , \; \sigma_E^2) \label{expunc}
\ee
The notation\footnote{Note that the subscript, $i$, has been dropped merely to reduce the notational clutter.}
$E \; | \; \theta$ means that $E$ is conditional on a particular value of $\theta$.
This is the usual way of defining a likelihood function.
It is convenient to use the so-called delta method\footnote{Given the random variable $X \sim N(\mu,\sigma^2)$, the
delta method~\cite{Oehlert:1992} provides a way to estimate the distribution of a function of $X$:
$$g(X) \sim N \left( g(\mu) + g''(\mu) \, \sigma^2/2 \, , \, (g'(\mu) \, \sigma)^2\right)$$} to obtain the approximate distribution
\be
   \ln E \; | \; \theta \sim N \left( \ln \theta - \frac{\widetilde{\sigma}_E^2}{2} \, , \,\widetilde{\sigma}_E^2 \right) \label{eeq}
\ee
The purpose of applying the natural log to the random variable is so that its variance can be expressed in terms of the
relative uncertainty, $\widetilde{\sigma}_E=\sigma_E/\theta$. This is convenient because it is assumed that the relative
uncertainty is constant for each quantity of interest. The quantities and uncertainty values are listed in Table~\ref{Uncertainty}.

It cannot be assumed, as in the case of the experimental measurements, that the model predictions have no systematic bias. Instead,
it is assumed that the model predictions are normally distributed about the true values
multiplied by a bias factor, $\delta$:
\be
   M \; | \; \theta \sim N \left(\delta \, \theta \, , \, \sigma_M^2 \right) \label{mdist}
\ee
The standard deviation, $\sigma_M$, is the model-intrinsic uncertainty, {\em i.e.} model error.
This and the bias factor, $\delta$, are the parameters that are sought.
Again, the delta method renders a distribution for $\ln M$ whose parameters can be expressed in terms of a
relative standard deviation:
\be
   \ln M \; | \; \theta \sim N \left(\ln \delta +\ln \theta - \frac{\widetilde{\sigma}_M^2}{2} \; , \;
   \widetilde{\sigma}_M^2 \right) \quad ; \quad \widetilde{\sigma}_M=\frac{\sigma_M}{\delta \, \theta} \label{meq}
\ee
Combining Eq.~(\ref{eeq}) with Eq.~(\ref{meq}) yields:
\be
   \ln M  - \ln E \sim N \left( \ln \delta - \frac{\widetilde{\sigma}_M^2}{2}+\frac{\widetilde{\sigma}_E^2}{2} \; ,
   \; \widetilde{\sigma}_M^2+\widetilde{\sigma}_E^2 \right) \label{lnMeq}
\ee
What is now needed is a way to estimate the mean and standard deviation of this combined distribution. First, define:
\be
   \overline{\ln M} = \frac{1}{n} \, \sum_{i=1}^n \, \ln M_i  \quad ; \quad
   \overline{\ln E} = \frac{1}{n} \, \sum_{i=1}^n \, \ln E_i
\ee
The least squares estimate\footnote{Note that $\hat{\sigma}$ denotes an estimate of $\sigma$.} of the standard
deviation of the combined distribution is defined as:
\be
   \widehat{ \widetilde{\sigma}_M^2 } + \widetilde{\sigma}_E^2  = \frac{1}{n-1} \sum_{i=1}^n \,
   \left[ \left(\ln M_i - \ln E_i \right) - \left( \overline{\ln M} - \overline{\ln E} \right)  \right]^2 \label{stdev}
\ee
Recall that $\widetilde{\sigma}_E$ is known and the expression on the right can be evaluated using the pairs of measured and
predicted values. An estimate of $\delta$ can be found using the mean of the distribution:
\be
   \hat{\delta} = \exp \left( \overline{\ln M}-\overline{\ln E}+\frac{ \widehat{\widetilde{\sigma}_M^2}}{2}-\frac{\widetilde{\sigma}_E^2}{2} \right)
\ee
Taking the assumed normal distribution of the model prediction, $M$, in Eq.~(\ref{mdist}) and using
a Bayesian argument\footnote{The form of Bayes theorem used here states that the posterior distribution is the product of
the prior distribution and the likelihood function, normalized by their integral:
$f(\theta|M)= p(\theta) \, f(M|\theta)/\int p(\theta) \, f(M|\theta) \, d\theta$.
A constant prior is also known as a Jeffreys prior~\cite{Gelman:Stats}.}
with a non-informative prior for $\theta$, the posterior distribution can be expressed:
\be
   \delta \, \theta \; | \; M \sim N \left( M \; , \; \sigma_M^2 \right) \label{thetaeq}
\ee
The assumption of a non-informative prior implies that there is not sufficient information about the
prior distribution ({\em i.e.} the true value) of
$\theta$ to assume anything other than a uniform\footnote{A uniform distribution means that for any two equally sized intervals of the real line,
there is an equal likelihood that the random variable takes a value in one of them.} distribution.
This is equivalent to saying that the modeler has not biased the model input parameters to compensate for a known
bias in the model output. For example, if a particular model has been shown to over-predict compartment temperature, and the modeler has reduced the specified heat release
rate to better estimate the true temperature, then it can no longer be assumed that the prior distribution of the true temperature is uniform.
Still another way to look at this is by analogy to target shooting. Suppose a particular rifle
has a manufacturers defect such that, on average, it shoots 10~cm to the left of the target. It must be assumed that any given shot by a marksman without this knowledge is
going to strike 10~cm to the left of the intended target. However, if the marksman knows of the defect, he or she will probably aim 10~cm to the right of the
intended target to compensate for the defect. If that is the case, it can no longer be assumed that the intended target was 10~cm to the right of the bullet hole.

In summary, assuming that the modeler has not modified the input parameters to compensate for a known bias in the model,
the true value of the model output quantity, $\theta$, given the model prediction, $M$, is assumed to be normally distributed with the following mean and standard deviation:
\be
   \theta \; | \; M \sim N \left( \frac{M}{\hat{\delta}} \; , \; \widehat{\widetilde{\sigma}_M^2} \left( \frac{M}{\hat{\delta}} \right)^2 \right) \label{truth}
\ee
This formula has been obtained by dividing by the bias factor, $\delta$, in Eq.~(\ref{thetaeq}).
Below, there is an example of how one might make use of this formula in practice.
First, however, the accuracy of the procedure just described needs to be verified.

\section{Verifying the Procedure}

The statistical analysis described in the previous section is difficult to understand without a fairly good background in Bayesian analysis. However,
the calculation itself is no more difficult than determining means and standard deviations of a few columns of numbers and it can be easily done with
a simple spreadsheet program.

To better illustrate the process, and also to verify this procedure, start with 1000 uniformly distributed
random numbers, $\theta_i$, between 0 and 1000. These numbers represent a particular quantity of interest, like a gas temperature at a particular
point and at a particular time, and it is assumed that these values
have no uncertainty -- they represent the ``truth.'' Of course, the true values can never be known, but for this hypothetical exercise they are assumed.
Next, with $\theta_i$ as the mean, a normally-distributed random variable is chosen that represents a hypothetical measurement, $E_i$.
The relative standard deviation of the distribution, $\widetilde{\sigma}_E$, is assumed known. In the same way, a normally-distributed
random variable is chosen that represents a hypothetical
model prediction, $M_i$. The mean of distribution $\delta \, \theta_i$ and the relative standard deviation, $\widetilde{\sigma}_M$,
is specified.
This procedure creates 1000 pairs of $(E_i,M_i)$ with which the procedure outlined in the previous section can be tested.
Using the 1000 pairs of values and the experimental uncertainty, $\widetilde{\sigma}_E$, the specified model bias, $\hat{\delta}$,
and relative error, $\widetilde{\sigma}_M$, of the hypothetical model predictions should be accurately estimated.

Two examples are considered. For the first example, assume that the model has no bias ($\delta=1$) and that the relative uncertainty of the measurements and the
relative error of the model are both 0.1, or 10~\%. The scatter plot on the left side of Fig.~\ref{Case_1_Scatter} displays the model predicted values compared to the
true values. The dashed lines indicate the 95~\% confidence interval; that is, it is expected that 95~\% of the points should fall between these
lines, whose slopes are plus and minus 20~\% ($2\, \widetilde{\sigma}_M$) of the diagonal line. Of course, this plot cannot exist in a real situation, because the true
values are never known. Instead, the only way to present the data is via the scatter plot on the right side of Fig.~\ref{Case_1_Scatter}. Here, the measured values, $E_i$, are
compared with the predicted values, $M_i$. The same dashed lines are carried over from the plot on the left. Because the predicted values are being compared
with measurements that have uncertainty, it appears that the model error is greater than it actually is because the data points are now scattered noticeably beyond the
original uncertainty bounds. In this hypothetical example, the model is
assumed to be as accurate as the measurements, yet the comparison makes it seem as if the model is less accurate than the experiments. The procedure outlined
above, which makes use of only the measured and predicted values, is able to extract from the data the fact that the hypothetical model has a
relative error of 10~\%, not the roughly 15~\% that one would infer from the plot if the experimental uncertainty were not taken into account.

\begin{figure}[t]
\begin{center}
\includegraphics[height=3.2in]{FIGURES/Case_1_Scatter_T_vs_M}
\includegraphics[height=3.2in]{FIGURES/Case_1_Scatter_E_vs_M}
\end{center}
\caption[Verification of the model error calculation, Case 1.]{(Left) A comparison of predicted and ``true'' values for 1000 hypothetical
experiments in which the model predictions and experimental measurements have the same accuracy.
(Right) The same data, except now the predicted values are compared to measured values. On both plots, the uncertainty bounds apply to both
the predicted and measured values.}
\label{Case_1_Scatter}
\end{figure}


For the second example, the hypothetical model predictions and the experimental measurements are more realistic. Assume now that the
measured quantity is surface temperature, and that the relative uncertainty of the measured values, $\widetilde{\sigma}_E=0.07$, is obtained from Table~\ref{Uncertainty}.
We assume the model has a bias factor of 1.03 and its relative error is 0.10 in order to generate a set of hypothetical model predictions.
A graphical representation of the data is shown in Fig.~\ref{Case_2_Scatter}. Note that the
experimental uncertainty bounds are represented by solid lines, and the model bias and relative error are represented by the dashed lines.
Each are expressed as 95~\% confidence intervals ($2 \, \widetilde{\sigma}$).

\begin{figure}[t]
\begin{center}
\includegraphics[height=3.2in]{FIGURES/Case_2_Scatter_T_vs_M}
\includegraphics[height=3.2in]{FIGURES/Case_2_Scatter_E_vs_M}
\end{center}
\caption[Verification of the model error calculation, Case 2.]{(Left) A comparison of predicted and ``true'' values for 1000 hypothetical
experiments. The short dashed lines indicate the bias, $\delta=1.03$, and the relative error, $2\widetilde{\sigma}_M=0.20$, of the model predictions.
The solid, off-diagonal lines indicate the relative uncertainty, $2\widetilde{\sigma}_E=0.14$ of the experimental measurements.
(Right) The same data, except now the predicted values are compared to measured values. On both plots, the uncertainty bounds are the same.}
\label{Case_2_Scatter}
\end{figure}

The statistical procedure outlined above ought to provide estimates of the values of the bias factor, $\delta$, and the model error, $\widetilde{\sigma}_M$, for the
hypothetical model predictions. The results of seven random trials are shown in Table~\ref{trials}. For each trial, values representing the model
predictions and experimental measurements were randomly selected based on the assumed distributions. The calculation procedure discussed above was applied to these
hypothetical values. The resulting estimates of the model bias and relative error were not exactly the same as those used to generate the data
because the calculation procedure relies on truncated Taylor series approximations.
However, given the fact that the experimental uncertainty estimate,
$\widetilde{\sigma}_E$, is often only a gross approximation in its own right, the accuracy of the procedure is more than adequate, as indicated by the average values
of the seven trials.

\begin{table}[t]
\caption{Estimated bias and relative error from random trials used to verify the analysis. }
\begin{center}
\begin{tabular}{|c|c|c|}
\hline
Trial   & Bias Factor      & Relative Error \\ \hline \hline
Exact   & 1.030            &    0.100            \\ \hline \hline
1       & 1.031            &    0.097            \\ \hline
2       & 1.028            &    0.101            \\ \hline
3       & 1.026            &    0.108            \\ \hline
4       & 1.034            &    0.100            \\ \hline
5       & 1.027            &    0.105            \\ \hline
6       & 1.035            &    0.100            \\ \hline
7       & 1.029            &    0.097            \\ \hline \hline
Average & 1.030            &    0.101            \\ \hline
\end{tabular}
\end{center}
\label{trials}
\end{table}





\section{Making Use of the Model Error}

The previous sections describes a method of quantifying the model error by comparing its predictions with experimental measurements. But how does one make use of the computed
model bias and relative error? This is best answered with an example. Suppose the model is being used to estimate the likelihood that
electrical control cables could be damaged due to
a fire in a compartment. Damage is assumed to occur when the surface temperature of any cable reaches 400~$^\circ$C. It is also assumed that the fire is
ignited within an electrical cabinet and the heat release rate of the fire is a specified function of time, and that all other input
parameters for the model are known and provided. Finally, it is assumed, for the time being, that there is no uncertainty
associated with any of these assumptions. What is the likelihood that the cables would be damaged if that fire were to occur? The calculation is performed, and the
model predicts that the maximum surface temperature of the cables is 350~$^\circ$C. Does this mean that there is no chance of damage, assuming that the input parameters
and assumptions are not in question at the moment? The answer is no, because the model itself is subject to error. So what is the chance that the
cables could actually reach temperatures greater than 400~$^\circ$C?

Before answering this question, first consider past experiments for which model predictions have been compared to measured surface temperatures of objects
with similar thermal characteristics as the cables in question. How ``similar'' the experiment is to the hypothetical scenario under study can be quantified by way of
various parameters, like the thermal inertia of the object, the size of the fire, the size of the compartment, and so on. Next, the results of the validation study can be
analyzed following the procedure spelled out above, which provides an estimate of the bias factor, $\delta$, and relative error, $\tilde{\sigma}_M$, for the model
predictions of this particular quantity. For the sake of argument, assume a bias factor is 1.03; that is, on average, the model has been shown to over-predict
surface temperatures by 3~\%. Also assume that the relative error has been calculated and it is 0.10~. These are the same values that were assumed in the second example
of the previous section.
Now, consider the graph shown in Fig.~\ref{bell_curve}.
The vertical lines indicate the ``threshold'' temperature at which damage is assumed to occur (400~$^\circ$C), and the temperature predicted by the
model (350~$^\circ$C). Given an ambient temperature of 20~$^\circ$C, the predicted temperature rise, $M$, is 330~$^\circ$C.
The bell curve is taken as a normal distribution whose mean and standard deviation are obtained from Eq.~(\ref{truth}) and calculated here:
\be \mu = 20 + \frac{M}{\delta} = 20 + \frac{330}{1.03} = 340.4 \; ^\circ \hbox{C}  \quad ; \quad
   \sigma = \widetilde{\sigma}_M \, \frac{M}{\hat{\delta}} = 0.10 \times \frac{330}{1.03} = 32 \; ^\circ \hbox{C}  \ee
respectively. The shaded area beneath the bell curve is the probability (0.03 in this case) that the ``true'' temperature can exceed 400~$^\circ$C.
This means that there is a 3~\% chance that the cables could
become damaged {\em based solely on the fact that the model is not a perfect representation of reality.}

The obvious question to ask at this point is what if it cannot be assumed that the specified fire, material properties, and other input parameters are
known exactly? How does that affect our estimate of the likelihood of cable damage? The procedure above has only provided a way of expressing the model
error. What if the specified fire in the electrical cabinet is actually chosen from a distribution of heat release rates?
What if the material properties of the cables are not known exactly? Assuming one could quantify the uncertainty of all of the input parameters, and assuming that the model error
and the input uncertainty are uncorrelated, it is possible to combine the two following the procedure that was described above that is used to determine the combined
experimental uncertainty. Of
course, the uncertainty associated with the input parameters would need to be quantified and propagated through the model. The end result would be a widening of
the distribution shown in Fig.~\ref{bell_curve} and an increase in the likelihood of cable damage, assuming that the parameters used in the ``base case'' were
all taken as the mean values of their respective distributions. In fact, depending on the scenario, the uncertainty associated with the input parameters can far outweigh
the model error. For example, it was discussed above that the upper layer temperature in a compartment is proportional to the heat release rate to the two-thirds power.
Hamins~\cite{NUREG_1824} demonstrates that the surface temperature of an object also exhibits the same sensitivity to the HRR. Suppose that in a compartment fire
analysis, the HRR is chosen from a distribution derived from experimental measurements of the burning rate of the potential fuel sources in the room. Suppose a similar
analysis is done for the other input parameters that have a significant impact on the results. Assuming that the
input parameters are normally-distributed, and that the
simulation makes use of the mean values, the combined uncertainty in the result could
be expressed in terms of the model error, $\widetilde{\sigma}_M$, plus contributions from the uncertainty in the input parameters:
\be
   \widetilde{\sigma}^2 = \widetilde{\sigma}_M^2 + \sum_i p_i^2 \widetilde{\sigma}_i^2  \label{comb_unc}
\ee
The factors, $p_i$, represent the power dependencies of the individual input parameters. For example, a prediction of the surface temperature of an object has a power dependence of
$p=0.67$ on the HRR. In essence, Eq.~(\ref{comb_unc}) combines the sensitivity and error analyses to produce a single estimate of the total uncertainty of the
model result. To the person evaluating the analysis, like the AHJ, this is really all that matters. However, for the model developers and model users, it is useful to
decompose the total uncertainty into its constituent parts. That encourages the developers to reduce as much as possible the value of $\widetilde{\sigma}_M$ and the users to reduce
the values of $\widetilde{\sigma}_i$. It also indicates the major sources of uncertainty so that resources can be spent wisely addressing the most important ones.



\section{Limitations}

The above verification exercises are valuable in assuring that the procedure works as designed, but it also points out a few issues that need to be addressed. First, any
statistical procedure is based on the law of averages, or, in other words, more data is better than less. It is usually not possible to conduct a large number of
fire experiments
to assess the accuracy of the model in predicting each quantity of interest. Sometimes there are only a few data points with which to estimate the model error. Worse yet,
there may not be enough information about the experimental procedure to estimate the uncertainty of the reported measurements. In such cases, it may be better
to simply present the comparison of model prediction and experimental measurement as a series of plots like Fig.~\ref{temp_history}, or in the form of a scatter plot
(Fig.~\ref{scatterplot}) without any uncertainty or error bars. The value of the validation process outlined above is that it is
possible at any step to stop and accept the raw output of the study as the basis for making an assessment of the model.
The uncertainty analysis and error quantification is valid only to
the extent that sufficient experimental data with quantified uncertainty estimates is available.
It does more harm than good to attempt to quantify the model error
with insufficient means to do so.

Another concern with the above procedure is that an over-estimate of the experimental uncertainty will result in an under-estimate of the model error. Keep
in mind that the model can never be declared more accurate than the experimental measurements against which it is compared. This rule is demonstrated
mathematically by Eq.~(\ref{stdev}) where it is observed that an over-estimate of the experimental uncertainty, $\widetilde{\sigma}_E$, can result in
the square root of a negative number, an imaginary number. In the case that the computed model error is less than the estimated experimental uncertainty, the
latter must be re-evaluated, or the number of data points needs to be seriously questioned.



\section{Conclusion}

A procedure has been proposed to estimate model error by way of comparisons of model predictions with experimental measurements whose uncertainty has been
quantified. For clarity of presentation, issues associated with the selection of experiments, metrics of comparison, and presentation of results, have not been
discussed in detail because these decisions are application-specific and best left to the end user or AHJ.
