
\chapter{Quantifying Model Error}

Model predictions are subject to two forms of uncertainty, {\em intrinsic} and {\em extrinsic}. Extrinsic uncertainty refers to the measured or postulated parameters that constitute the
model input. Intrinsic uncertainty refers to the model itself.
For the sake of clarity, we will use the word {\em uncertainty} in regard to the measured model inputs, and the word {\em error} in regards to the mathematical model itself.
The reason for this is two-fold. For one,
it helps to differentiate various terms that are used to quantify model accuracy; but, more importantly, it highlights the subtle distinction between the roles of the model
and of the modeler. For example, if the modeler chooses 10~MW to be the heat release rate of a hypothetical fire, we do not say it is wrong, but rather uncertain because there
is no easy way to know for sure just how big the fire is going to be. The model, on the other hand, includes assumptions and approximations that make its predictions less
accurate for the sake of practicality. If the model is well-designed, the error associated with its formulation will be relatively small. Exactly how small is the subject of
this chapter.

\section{The Validation Process}

A fire model validation study typically consists of comparing a point measurements made during a compartment fire experiment with corresponding model predictions.
Figure~\ref{temp_history} is a typical result of such an exercise, and given that usually dozens of point measurements are made during each experiment,
and potentially dozens of experiments are conducted as part of a test series, we can expect hundreds of such plots for any given quantity of interest. In many cases, these
plots in themselves provide sufficient information for a potential model user or authority having jurisdiction (AHJ) to decide if the model is sufficiently accurate for the
given application. Because there is no consensus ``acceptance criteria'' within the fire protection community to indicate what constitutes sufficient accuracy, the
judgment is left to end user or AHJ. It can be argued that this is as it should be because each application is different. For example, some regulatory
authorities prefer models that consistently over-predict the severity of the fire, and as such the model serves as a ``screening tool.'' Design engineers, however,
usually want the model to predict as accurately as possible, even though they may apply some form of safety factor to the results.
In any case, there is certainly no reason why we cannot develop a process to quantify the accuracy of the model that goes beyond simply
publishing hundreds of plots like the one shown in Fig.~\ref{temp_history}.

\begin{figure}[ht]
\begin{center}
\includegraphics[height=2.5in]{FIGURES/sample_time_history}
\end{center}
\caption[Sample time history plots.]{Example of a typical time history comparison of model prediction and experimental measurement.}
\label{temp_history}
\end{figure}

To this end, we first have to decide how to condense all the data represented by these plots into a more tractable form. Specifically, we
have to decide on a metric with which to compare two curves like the ones shown in Fig.~\ref{temp_history}. Peacock {\em et al.}~\cite{Peacock:FSJ1999} discuss various possible metrics. The most commonly used metric is simply to compare the peak measured value against the peak
predicted value. If the data is spiky, some form of time-averaging can be used. Regardless of the exact form of the metric, what results from
this exercise is a pair of numbers for each plot, $(E_i,M_i)$, that can be depicted graphically as shown in Fig.~\ref{scatterplot}. Now, at least, we have
condensed hundreds of plots into just one, but we still have the problem of quantifying the degree of scatter in the results. At this stage in the analysis, the
diagonal line on the plot shown in Fig.~\ref{scatterplot} only indicates where a prediction and measurement agree. But because the model has error, and
the measurement has uncertainty, it cannot be said that the model is perfect if its predictions agree exactly with measurements. There needs to be a way of quantifying
both the error and uncertainty before any conclusions can be drawn.
Such an exercise would result in the error/uncertainty bars shown in the figure. At first glance, it would appear that the
horizontal bar associated with each point represents the uncertainty in the measurement, and the vertical bar represents the error in the model. Unfortunately, it
is not a simple task to quantify these values, nor can it be said that the vertical bar is the result solely of model error. As will be discussed below, any
experiment involves measurements of elevated temperatures, heat flux, {\em etc.}, resulting from the fire, but also measurements of parameters, like the
heat release rate and material properties, that are then fed into the model as input. Uncertainty in these input parameters combines with the error in the model
itself to form the vertical uncertainty/error bar associated with each point in Fig.~\ref{scatterplot}. Decoupling the error from the uncertainty
is the subject of analysis below. Before presenting this
analysis, however, we must first discuss in more detail what we mean by experimental uncertainty and model error.

\begin{figure}[ht]
\begin{center}
\includegraphics[height=3.in]{FIGURES/scatterplot}
\end{center}
\caption[Sample scatter plot.]{Example of a typical scatter plot of model predictions and experimental measurements.}
\label{scatterplot}
\end{figure}


\section{Model Error}

A computational fluid dynamics (CFD) model like FDS is essentially a numerical algorithm that solves the Navier-Stokes equations.
However, it does not actually solve the Navier-Stokes equations, but rather an approximate form of these equations. The approximation involves simplifying
physical assumptions, like the low-Mach number approximation and the large eddy simulation (LES) technique of representing subgrid-scale turbulence.
But the most critical approximation is the discretization of the governing equations. For example, the partial derivative of the density, $\rho$,
with respect to the coordinate direction, $x$, is written in approximate form as:
\be \frac{\partial \rho}{\partial x} = \frac{\rho_{i+1} - \rho_{i-1}}{2 \, \dx} + \mathcal{O}(\dx^2) \ee
where $\dx$ is the grid spacing chosen by the model user.
The second term on the right represents all the terms of order $\dx^2$ and higher in the Taylor series expansion and are known collectively as the
{\em discretization error}. These extra terms are simply dropped from
the equation set, the argument being that they become smaller and smaller with decreasing grid cell size, $\dx$. The effect of these neglected terms is captured, to
some extent, by the subgrid-scale turbulence model, but that is yet another approximation of the real thing. What effect do these approximations have on
the predicted results? It is very difficult to determine based on an analysis of the discretized equations. At best, one can demonstrate convergence towards some
analytical solution of the non-turbulent Navier-Stokes equations at a rate determined by the spatial and temporal differencing schemes, but this still does not answer a
question like, ``What is the error of an FDS prediction of the gas temperature at a particular location in the room at a particular point in time?''

To make matters worse, there are literally dozens of subroutines that make up a model like FDS, from its radiation solver, solid phase heat transfer routines, pyrolysis model,
empirical mass, momentum and energy transfer routines at the wall, and so on. A careful, systematic error analysis of the combination of all these routines is
nearly impossible. Indeed the complexity of the analysis would approach that of FDS itself. For this reason, we rely on comparisons of FDS predictions to as many
experiments as possible as a means of quantifying model error. That is the primary focus of this entire guide, and the first step in the process of quantifying the
model error is to quantify the uncertainty of the measurements against which the model predictions are to be compared. This is the subject of the next section.


\section{Experimental Uncertainty}

The test reports for the experiments described in the previous chapter include little uncertainty analysis, in general. Quite a
few include none whatsoever.
However, in order to assess the accuracy of the model whose predictions have been compared to these measurements, there must be some estimate of the
combined effect of the uncertainty in the reported experimental conditions, like the heat release rate of the fire,
and the reported measurements of the quantities of interest, like the hot gas layer (HGL)
temperature or heat flux. Otherwise, the difference between model and measurement would simply have to be attributed as model error and there is no point in
continuing this exercise.

Rather than throwing out several decades worth of fire test data, there are ways that we can estimate the uncertainty of the measurements based on work
that has been done to quantify the accuracy of such devices as heat flux gauges, oxygen-depletion calorimeters, thermocouples and gas sensors.
In a recent fire model validation study conducted by the U.S.~Nuclear Regulatory Commission~\cite{NUREG_1824}, Hamins estimated the combined
uncertainty of the quantities of interest for the large scale fire experiments under consideration. Referring to Fig.~\ref{scatterplot}, there were
two uncertainty estimates needed for each quantity. The first was an estimate, expressed in the form of a 95~\% confidence interval, of the
uncertainty in the measured quantity itself. For example, reported gas and surface temperatures were made with thermocouples of various designs (bare-bead,
shielded, aspirated) with different size beads, metals, and so on. For each, one can estimate the uncertainty in the reported measurement. Next, the
uncertainty of the measurements of the reported conditions was estimated, including the heat release rate, leakage area, ventilation rate, material
properties, and so on. The effect of these uncertainties on the reported measured quantity had to be assessed as well. For example, most of the
experiments used oxygen consumption calorimetry as the means of measuring the heat release rate.
Estimates of the uncertainty in large scale calorimeters is on the order of 15~\%. It has been shown~\cite{SFPE:Walton} that the hot gas layer temperature rise due to
a compartment fire is proportional to the HRR raised to the two-thirds power, thus a 15~\% uncertainty in the HRR would lead to a $2/3 \times 15$~\%=10~\%
uncertainty in the model prediction. This uncertainty now needs to be combined with the uncertainty in the thermocouple measurement itself. Let us say that
this is also 10~\%. Because the two forms of uncertainty are uncorrelated, they are combined by quadrature (summing of squares) to yield a combined
uncertainty of 14~\%. An other way to look at this is to recognize that the combined uncertainty is represented as the diagonal of the rectangle formed
from the horizontal and vertical uncertainty/error bars in Fig.~\ref{scatterplot}.

Hamins performed this exercise for ten quantities of interest in the U.S. NRC validation study. The results are summarized in Table~\ref{Uncertainty}, with
each combined uncertainty reported in the form of a 95~\% confidence interval ({\em i.e.} $2 \, \widetilde{\sigma}_E$). The tilde above the $\sigma$ denotes a
{\em relative uncertainty}, which is a convenient way to report it because we are assuming that the uncertainty in the reported value is proportional to
its magnitude. This assumption is made throughout the analysis for both measurement uncertainty and model error. The assumption is based on a
qualitative assessment of dozens of scatter plots similar to that shown in Fig.~\ref{scatterplot} that show the scattered points to form an expanding ``wedge''
about the diagonal line, or some other off-diagonal line due to an assumed bias in the model predictions. This assessment is a critical component of the
analysis described in the next section.

\begin{table}[ht!]
\caption{Summary of Hamins' uncertainty estimates~\cite{NUREG_1824}. }
\begin{center}
\begin{tabular}{|l|c|}
\hline
Measured Quantity               & Combined Relative       \\
                                & Uncertainty, $2 \, \widetilde{\sigma}_E$ (\%)       \\ \hline \hline
HGL Temperature                 & 14    \\ \hline
HGL Depth                       & 13    \\ \hline
Ceiling Jet Temperature         & 16    \\ \hline
Plume Temperature               & 14    \\ \hline
Gas Concentrations              & 9     \\ \hline
Smoke Concentration             & 33    \\ \hline
Pressure with Ventilation       & 80    \\ \hline
Pressure without Ventilation    & 40    \\ \hline
Heat Flux                       & 20    \\ \hline
Surface Temperature             & 14    \\ \hline
\end{tabular}
\end{center}
\label{Uncertainty}
\end{table}


\section{Calculating Model Error}

We are now ready to describe a method of extracting an estimate of the model error from a set of measured and predicted values, with only an estimate of
the combined experimental uncertainty as a way to characterize the accuracy of the measurements.
The set of model predictions and the corresponding set of experimental measurements are denoted
$M_i$ and $E_i$, respectively, where $i$ ranges from 1 to $n$. Without loss of generality, assume that the
ambient value of these quantities is zero. With this information, {\em and only this information}, we seek to quantify the model error.

In addition to assuming that the experimental uncertainty and the model error can be expressed in relative terms, we also assume that
the experimental and model data is normally distributed. Given the complexity of both the model and the experiment, it is hard to justify
any particular distribution. In fact, the very reason why we have developed this method of quantifying model error based on validation
experiments is because the model algorithm itself is too complicated to work with directly as a means of estimating the error.

With these assumptions in mind, we proceed with the statistical analysis of the data.
First, assume that $E|\theta \sim N(\theta,\sigma_E^2)$, meaning that the probability distribution\footnote{$N(\mu,\sigma^2)$ denotes a normal (Gaussian) distribution
with mean, $\mu$, and standard deviation, $\sigma$.}  of $E$
is conditional on a particular value of $\theta$. This is the usual way of defining a likelihood function. The parameter $\theta$ is the ``true'' value of the quantity under
consideration. In short, we are assuming that the experimental measurement has no bias and its standard deviation, $\sigma_E$, has been
determined following the procedure described in the
previous section. Table~\ref{Uncertainty} lists the relative experimental uncertainty, $\widetilde{\sigma}_E=\sigma_E/\theta$, for the various quantities of interest.
In the analysis to follow, it is convenient to work with $\ln(E)$ because the standard deviation of $\ln(E)$ is approximately $\widetilde{\sigma}_E$.
We can use the usual delta method~\cite{Oehlert:1992} to obtain the approximate distribution $\ln(E)|\theta \sim N(\ln(\theta),\widetilde{\sigma}_E^2)$.
Using a Bayesian argument with a non-informative prior on $\ln(\theta)$, we obtain the posterior distribution as $\ln(\theta) \sim N(\ln(E),\widetilde{\sigma}_E^2)$ .

Next, assume that $M|\theta \sim N(\delta \, \theta,\sigma_M^2)$ , where $\delta$ is a systematic multiplicative bias and $\sigma_M$ is the model-intrinsic uncertainty,
{\em i.e.} model error.
Again, using the log transformation we obtain $\ln(M)|\theta \sim N(\ln(\delta)+\ln(\theta)-\omega_M^2/2,\omega_M^2)$
where $\omega_M=\sigma_M/(\theta \, \delta)$. Note that the estimate of the mean includes an extra term in the Taylor series expansion to obtain a more precise estimate of
$\delta$. Because $\ln(\theta)$ is random, and we have its posterior distribution from above, we can obtain the marginal density
(that is, for any $\theta$) of $\ln(M) \sim N(\ln(\delta)+\ln(E)-\omega_M^2/2,\omega_M^2+\widetilde{\sigma}_E^2)$.

What we need now is a way to estimate $\delta$, the bias factor, and $\omega_M$, the relative error, of the model predictions. First, define:
\be \overline{M} = \frac{1}{n} \, \sum_{i=1}^n \, \ln(M_i)  \quad ; \quad \overline{E} = \frac{1}{n} \, \sum_{i=1}^n \, \ln(E_i) \ee
Next, define
\be \widehat{M}_i = \overline{M}-\overline{E} + \ln(E_i) \ee
The least squares estimate of the standard deviation of $\ln(M_i)$ is
\be u(M) = \sqrt{\omega_M^2+\widetilde{\sigma}_E^2} \approx \sqrt{ \frac{1}{n-1} \sum_{i=1}^n \, \left[ \ln(M_i)-\widehat{M}_i \right]^2 } \ee
Note that $u(M)$ is the {\em total relative uncertainty} of $M$.
Next, compute
\be \widehat{\omega}_M = \sqrt{u(M)^2-\widetilde{\sigma}_E^2} \label{model_error} \ee
An estimate of $\ln(\delta)$ is $\overline{M}-\overline{E}+\widehat{\omega}_M^2/2$, and, thus, the estimate of $\delta$ is
\be \widehat{\delta} = \exp(\overline{M}-\overline{E}+\widehat{\omega}_M^2/2) \ee
The model-intrinsic component, {\em i.e.} the model error, $\widetilde{\sigma}_M$, can be estimated as follows
\be \widetilde{\sigma}_M = \widehat{\delta} \, \widehat{\omega}_M \label{model_error2} \ee

In essence, this procedure gives us a way to estimate the ``true'' value, $\theta$, using only a predicted value, $M$, and the bias factor and relative error
for that particular quantity that have been obtained using past validation experiments. The ``truth'' is a random variable with the following distribution:
\be \theta \sim N \left( M/\delta,(M \, \widetilde{\sigma}_M/\delta)^2 \right) \label{truth} \ee
Below, we describe how one might make use of this formula in practice. First, however, we need to verify the accuracy of the procedure just described.

\section{Verifying the Procedure}

The statistical analysis described in the previous section is difficult to understand without a fairly good background in Bayesian analysis.
However, the calculation itself is no more difficult than determining means and standard deviations of a few columns of numbers and it can be easily
done with a simple spreadsheet program.
In fact, to verify this procedure, we generate 1000 random numbers between 0 and 1000. We say that these numbers represent the ``true'' temperatures, $\theta_i$, from 1000 fire
experiments. Of course, we would never know the true temperatures -- this is only done for verification purposes. Next, for each $\theta_i$, we randomly choose an
experimental measurement, $E_i$, from a normal distribution whose mean is $\theta_i$ and whose relative uncertainty, $\widetilde{\sigma}_E=0.07$, obtained from Table~\ref{Uncertainty},
``HGL Temperature.'' Finally, for each $\theta_i$ we randomly choose a model prediction, $M_i$, from a normal distribution whose mean is $1.05 \, \theta_i$
and whose relative error is 0.15. In other words, we create a hypothetical set of measurements and predictions with known distributions.
What we have created are 1000 pairs of $(E_i,M_i)$ based on a given experimental uncertainty, $\widetilde{\sigma}_E$, and an assumed set of ``true'' values. A graphical
representation of the data is shown in Fig.~\ref{scatterplot_1000}. Note that the experimental uncertainty bounds are represented by thick, dashed lines, and the model
bias and relative error is shown by the lighter, dashed lines. Each are expressed as 95~\% confidence intervals ($2 \, \widetilde{\sigma}$).

\begin{figure}[ht]
\begin{center}
\includegraphics[height=3.0in]{FIGURES/scatterplot_1000}
\end{center}
\caption[Scatter plot for verification of model error calculation.]{Scatter plot showing 1000 pairs of experimental measurements and model predictions. The
experimental uncertainty is represented by the solid diagonal and thicker dashed lines. The model error is represented by the thinner diagonal and dashed lines.}
\label{scatterplot_1000}
\end{figure}

The statistical procedure outlined above ought to provide estimates of the values of the bias factor, $\delta$, and the model error, $\widetilde{\sigma}_M$, whose
values we used in creating the data in the first place. The results of six random trials are shown in Table~\ref{trials}. It is not expected that the
estimates will be exact because they are based on truncated Taylor series approximations. However, given the fact that the experimental uncertainty estimate,
$\widetilde{\sigma}_E$, is often only a gross approximation in its own right, the accuracy of the procedure is more than adequate.

\begin{table}[ht!]
\caption{Estimated bias and relative error from six trials used to verify the analysis. }
\begin{center}
\begin{tabular}{|c|c|c|}
\hline
Trial   & Bias Factor      & Relative Error \\ \hline \hline
Exact   & 1.05             &    0.15            \\ \hline \hline
1       & 1.045            &    0.1605            \\ \hline
2       & 1.053            &    0.1562            \\ \hline
3       & 1.056            &    0.1552            \\ \hline
4       & 1.060            &    0.1592            \\ \hline
5       & 1.060            &    0.1466            \\ \hline
6       & 1.053            &    0.1585            \\ \hline
\end{tabular}
\end{center}
\label{trials}
\end{table}



\section{Limitations}

The above verification exercise is valuable in assuring that the procedure works as designed, but it also points out a few issues that need to be addressed. First, any
statistical procedure is based on the law of averages, or, in other words, more data is better than less. It is usually not possible to conduct a large number of
fire experiments
to assess the accuracy of the model in predicting each quantity of interest. Sometimes we only have a few data points with which to estimate the model error. Worse yet,
we may not have the necessary information about the experimental procedure to estimate the uncertainty of the reported measurements. In such cases, it may be better
to simply present the comparison of model prediction and experimental measurement as a series of plots like Fig.~\ref{temp_history}, or in the form of a scatter plot
(Fig.~\ref{scatterplot}) without any uncertainty or error bars. The value of the entire validation process as outlined above is that it is possible at any step
along the way to simply stop and accept the raw output of the study as the basis for making an assessment of the model. The uncertainty analysis is useful only to
the extent that sufficient experimental data with quantified uncertainty estimates is available. It does more harm than good to attempt to quantify the model error
with insufficient means to do so.

Another concern with the above procedure is that an over-estimate of the experimental uncertainty will naturally result in an under-estimate of the model error. Keep
in mind that the model can never be declared more accurate than the experimental measurements against which it is compared. This rule is demonstrated
mathematically by Eq.~(\ref{model_error}) where it is observed that an over-estimate of the experimental uncertainty, $\tilde{\sigma}_E$, can result in
the square root of a negative number, an imaginary number.

\section{Making Use of the Model Error}

The previous sections describes a method of quantifying the model error by comparing its predictions with experimental measurements. But how does one make use of this
information? This is best answered with an example. Suppose the model is being used to estimate the likelihood that electrical control cables could be damaged due to
a fire in a particular compartment. Damage is assumed to occur when the surface temperature of any cable reaches 400~$^\circ$C. It is also assumed that the fire is
ignited within an electrical cabinet and the heat release rate of the fire is a specified function of time, and that all other input
parameters for the model are known and provided. Finally, it is assumed, for the time being, that there is no uncertainty
associated with any of these assumptions. What is the likelihood that the cables would be damaged if that fire were to occur? The calculation is performed, and the
model predicts that the maximum surface temperature of the cables is 350~$^\circ$C. Does this mean that there is no chance of damage, assuming that the input parameters
and assumptions are not in question at the moment? The answer is no, because we know that the model itself is subject to error. So what is the chance that the
cables could actually reach temperatures greater than 400~$^\circ$C?

Before we can answer this question, we need to consider past experiments for which model predictions have been compared to measured surface temperatures of objects
with similar thermal characteristics as the cables in question. How ``similar'' the experiment is to the hypothetical scenario under study can be quantified by way of
various parameters, like the thermal inertia of the object, the size of the fire, the size of the compartment, and so on. Next, the results of the validation study can be
analyzed following the procedure spelled out above, which provides us with a bias factor, $\delta$, and relative error, $\tilde{\sigma}_M$, for the model
predictions of this particular quantity. Let us say, for the sake of argument, that the bias factor is 1.05; that is, on average, the model has been shown to over-predict
surface temperatures by 5~\%. Let us also say that the relative error has been calculated and it is 0.15~.
Now, consider the graph shown in Fig.~\ref{bell_curve}.

\begin{figure}[ht]
\begin{center}
\includegraphics[width=5.in]{FIGURES/bell_curve}
\end{center}
\caption[Demonstration of model error.]{Plot showing how one might make use of the model error.}
\label{bell_curve}
\end{figure}

The vertical lines indicate the temperature at which damage is assumed to occur (400~$^\circ$C), and the temperature predicted by the
model (350~$^\circ$C). The bell curve is a normal distribution obtained from Eq.~(\ref{truth}) above. Its mean is 20+(350-20)/1.05 and its standard deviation
is 0.15(350-20)/1.05 . Notice that the statistical analysis
above assumes that the measured and predicted quantity, in this case surface temperature, is reported as a change above or below its ambient value, which is
20~$^\circ$C in this case. The shaded area beneath the bell curve is the probability (0.08 in this case) that the ``true'' temperature would exceed 400~$^\circ$C.
This means that there is an 8~\% chance that the cables could
become damaged {\em based solely on the fact that the model is not a perfect representation of reality.}

The obvious question to ask at this point is what do we do if we cannot assume that the specified fire, material properties, and other input parameters are
known exactly? How does that affect our estimate of the likelihood of cable damage? The procedure above has only provided a way of expressing the model
error. What about the so-called ``user effects''? What if the specified fire in the electrical cabinet is actually chosen from a distribution of heat release rates?
What if the material properties of the cables are not known exactly? Assuming one could quantify the uncertainty of all of the input parameters, and assuming that the model error
and the input uncertainty are uncorrelated, it is possible to combine the two following the procedure that was described above that is used to determine the combined
experimental uncertainty. Of
course, the uncertainty associated with the input parameters would need to be quantified and propagated through the model. The end result would be a widening of
the distribution shown in Fig.~\ref{bell_curve} and an increase in the likelihood of cable damage, assuming that the parameters used in the ``base case'' were
all taken as the mean values of their respective distributions. In fact, depending on the scenario, the uncertainty associated with the input parameters far outweighs
the model error.


\section{Conclusion}

This chapter outlines a procedure to estimate model error by way of comparisons of model predictions with experimental measurements whose uncertainty has been
quantified. For clarity of presentation, issues associated with the selection of experiments, metrics of comparison, and presentation of results, have not been
discussed in detail because these decisions are application-specific and best left to the end user or AHJ. The procedure is not a dramatic departure from current
methods of model validation, but it does present a very tractable method of distinguishing the model {\em error} from the {\em uncertainty} of the model inputs and the measurements
against which the model is compared. Too often these various forms of uncertainty are lumped together, in which case there is no way to know what part of the model, if
any, needs improvement.
