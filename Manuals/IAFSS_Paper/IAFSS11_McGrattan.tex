\documentclass[fleqn,b5paper]{article}

\usepackage{times,mathptm,helvet}
\usepackage{graphicx} % use \usepackage[demo]{graphicx} to suppress figures
\usepackage{pdfsync}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[justification=centering,labelsep=period,figurename=Fig.]{caption}
\usepackage[sort&compress]{natbib}
\usepackage{url}
\usepackage{hyperref}

\hypersetup{colorlinks=true, urlcolor=blue, citecolor=black, linkcolor=black}
\urlstyle{same}

\setlength{\paperwidth}{7.0in}
\setlength{\paperheight}{10.0in}
\setlength{\textwidth}{6.0in}
\setlength{\textheight}{9in}
\setlength{\topmargin}{-.5in}
\setlength{\headheight}{0in}
\setlength{\headsep}{0in}
\setlength{\parindent}{0in}
\setlength{\parskip}{.5\baselineskip}
\setlength{\footskip}{0.25in}
\setlength{\oddsidemargin}{-.5in}
\setlength{\evensidemargin}{-.5in}
\setlength{\mathindent}{0cm}
\setlength{\topsep}{0cm}
\setlength{\labelsep}{.8cm}
\pagestyle{empty}

\frenchspacing % single space between sentences
\widowpenalty=10000
\clubpenalty=10000
\raggedbottom

%% this changed in 2014, but I want to keep around in case they change back
%% define a special equation environment to put eq. numbers at 12.7 cm tab stop
%\newcounter{eqncounter}
%\def\theeqncounter{\arabic{eqncounter}}
%\newenvironment{iafssequation}{\refstepcounter{eqncounter}\begin{sloppypar}\vspace{.5\baselineskip}\noindent\begin{minipage}[s]{12.7cm}$\displaystyle}
%{$\hspace*{\fill}(\theeqncounter)\end{minipage}\vspace{.5\baselineskip}\end{sloppypar}}

\begin{document}

\renewcommand{\thefootnote}{\sf \arabic{footnote}}
\renewcommand{\d}{\mathrm{d}}
\renewcommand{\labelitemi}{{\scriptsize $\bullet$}}
\renewcommand{\labelitemii}{{\scriptsize $\bullet$}}

\newfont{\helvb}{phvb7t at 14pt} % helvetica narrow bold
\newfont{\helvn}{phvr7t at 10pt} % helvetica narrow

\bibliographystyle{unsrtnat}
\setcitestyle{numbers,citesep={,\!}} % cite style is [1,2] instead of [1, 2] (remove space)

\begin{flushleft}
{\helvb Fire Model Validation -- Eight Lessons Learned}\\
\hspace{1in}\\
\helvn KEVIN McGRATTAN\footnotemark[1], RICHARD PEACOCK\footnotemark[1], and KRISTOPHER OVERHOLT\footnotemark[1]  \\
\footnotemark[1] Fire Research Division \\
National Institute of Standards and Technology \\
Gaithersburg, Maryland, USA
\end{flushleft}
\rm

\vspace{\parskip}
{\bf ABSTRACT}

This paper provides a summary of a decade-long effort to verify and validate a variety of different fire models used in the commercial nuclear industry. The lessons learned apply to any industry in which fire models are used in a regulatory context. 

\vspace{\parskip}
{\bf KEYWORDS:} fire modeling; model validation; model uncertainty

\vspace{\parskip}
{\bf INTRODUCTION}

In 2007, the U.S.~Nuclear Regulatory Commission (NRC) and the Electric Power Research Institute (EPRI) published the results of a validation study of five different fire models commonly used by the commercial nuclear power industry~\cite{NUREG_1824}. The study was prompted by the NRC's adoption in 2004 of the National Fire Protection Association standard NFPA~805, {\em Performance-Based Standard for Fire Protection for Light-Water Reactor Electric Generating Plants}~\cite{NFPA_805}. In particular, NFPA~805 requires fire models to be {\em verified and validated}. The standard does not state explicitly what is meant by this. Guidance documents, like the SFPE {\em Guidelines for Substantiating a Fire Model for a Given Application}~\cite{SFPE_G.06}, and standards documents like ASTM~E~1355, {\em Standard Guide for Evaluating the Predictive Capabilities of Deterministic Fire Models}~\cite{ASTM:E1355}, and ISO~16730, {\em Assessment, Verification and Validation of Calculation Methods}~\cite{ISO16730}, all provide a basic framework for evaluating models. However, these documents do not have specific requirements as to how the model uncertainty is to be reported and how this information is to be used in a regulatory context. As a result, the NRC and EPRI took it upon themselves to develop a relatively simple framework for reporting and applying model uncertainty in day to day design analyses.

The primary lesson learned from this experience is that most validation studies reported in the literature are not particularly useful for regulatory officials. Some of the reasons for this are (1) journal articles invariably report results of older versions of the model, (2) the proposed application of the model is not always consistent with the application reported in the article, and (3) there is no simple method of applying the reported model uncertainty to the proposed application. Following the publication of the NRC validation study~\cite{NUREG_1824} in 2007, a member of the NRC review panel asked the simple question, ``if I use Model~X to predict Quantity~Y, how do I calculate uncertainty bounds?'' Because the application space of zone and CFD fire models is fairly wide, it is difficult to report the uncertainty of a given calculation in a simple and concise way. Often the regulator is given a list of journal articles attesting to the accuracy of the model, but there remains too much room for interpretation. As a result, the NRC and EPRI published a follow on guidance document, {\em Nuclear Power Plant Fire Modeling Analysis Guidelines}~\cite{NUREG_1934}, in which practical examples demonstrate how to apply the results of the model V\&V study. This paper highlights various aspects of these studies and the important lessons learned.


\vspace{\parskip}
{\bf Lesson 1: Model developers do not validate their models.}

A common misconception about model validation is that it is the responsibility of the model developers. Actually, it is the responsibility of the end users or regulatory authority working on their behalf. After all, to say that the model has been verified and validated means that it has been deemed acceptable for a particular use. The model developers might contribute examples demonstrating the model's reliability and accuracy, but they cannot make the decision as to whether the model is appropriate or not.

All this being said, it is unrealistic to believe that any organization will have the resources to thoroughly evaluate all aspects of a model, in particular a CFD model which has such a wide range of potential applications. Thus, the model developers do a considerable amount of work that others might review to make their assessment as to whether the model is appropriate or not. Model developers can assist in the process by organizing and documenting case studies that can be periodically updated as new versions of the model are developed. Anyone using the model should be able to examine the experimental report, input files, assumptions, and so on, to determine if the model is appropriate. 




\vspace{\parskip}
{\bf Lesson 2: Scientific journals are not the best means of reporting validation studies.}

Publication of technical results in peer-reviewed, archival journals is the most common form of scientific communication, and over the past thirty years, thousands of papers have been published in the fire literature documenting various forms of fire model V\&V. Standards like ASTM~E~1355 credit journal articles as part of the process of model evaluation:
\begin{quote}
The theoretical basis of the model should be subjected to a peer review $\ldots$ Publication of the theoretical basis of the model in a peer-reviewed journal article may be sufficient to fulfill this review.
\end{quote}
The drawback of journal publications, however, is that once the theoretical basis of the model has been established, the journal articles then become nothing more than a catalog of fairly routine verification exercises and validation studies. Worse, as new versions of the model are released, the journal articles can no longer be cited as the source of the quantified uncertainty results. It is simply impractical to publish again and again the results of validation studies; thus, a better way is needed to assure model users that the currently released version is of comparable or better accuracy than a previously documented version. This better way is for the model developers and interested end users to maintain and regularly update compilations of verification and validation results. This serves two purposes: (1) model users can cite the most recent V\&V results to justify their use of the model, and (2) model developers can be assured that changes to the source code do not adversely affect past results.

As a follow on to the 2007 NRC V\&V study documented in Ref.~\cite{NUREG_1824}, NIST issued validation guides for FDS and CFAST that follow the same general framework. These guides are automatically regenerated with each new release of the software so that the model uncertainty metrics described above are always appropriate for the current version.


\vspace{\parskip}
{\bf Lesson 3: Quantity makes up for lack of quality.}

Over the past 40 years, thousands of full-scale fire experiments have been performed at research and testing labs, universities, and consulting companies. Many of these experiments were performed specifically for model validation. In most cases, however, the test reports lack information needed for a complete assessment of the models. For example, experimental uncertainties, material property data, heat release rates, and other boundary conditions are often lacking. It is difficult to know exactly how the measurements were made and the data reduced. If the criteria for data selection is overly strict, there might not be any data left to use. Fortunately, what is lacking in quality can to some extent be made up in quantity. It is unwise to limit the model assessment to one or two sets of data from one laboratory.



\vspace{\parskip}
{\bf Lesson 4: Some things are easier to predict than others.}

There is no single metric that can answer the question, ``how accurate is the model?'' The accuracy depends on what is being predicted. For example, the average upper layer temperature in a pre-flashover fire scenario is a relatively easy quantity to predict. Empirical, zone and CFD models have all been shown to predict it nearly to within experimental uncertainty when the heat release rate is specified~\cite{NUREG_1824}. However, quantities such as heat flux, especially to targets near the fire, are more difficult to predict.

The first step of the validation process is the choice of output quantities. In fire safety analysis, typical output quantities are gas or target temperatures, height of the smoke layer within a compartment or the concentration of toxic species. The importance of the heat transfer calculations makes also the flow velocities and heat fluxes interesting quantities for validation. In some cases, the accurate prediction of pressure may be needed. In practical applications, many of the quantities are compared against some critical value monitoring the time to reach this value. Therefore, it may be necessary to validate the capability to predict the time-to-threshold, but usually this is not made because the time-to-threshold has no defined value if the threshold is never met.


\vspace{\parskip}
{\bf Lesson 5: Sophisticated comparison metrics may be more trouble than they are worth.}

A typical fire model validation study results in hundreds of point to point comparisons of predicted and measured temperatures, heat fluxes, gas concentrations, and so forth, all reported as time histories like the one shown in Fig.~\ref{temp_history}.
\begin{figure}[ht]
\begin{center}
\includegraphics[height=2.5in]{../FDS_Validation_Guide/FIGURES/sample_time_history}
\end{center}
\caption[Sample time history plots.]{Example of a typical time history comparison of model prediction and experimental measurement.}
\label{temp_history}
\end{figure}
Typically, the data is condensed into a more tractable form by way of a single metric with which to compare the two curves like the ones shown in Fig.~\ref{temp_history}. Peacock {\em et al.}~\cite{Peacock:FSJ1999} discuss various possible metrics. A commonly used metric is simply to compare the measured and predicted peak values. This was the choice of the NRC in performing their validation study~\cite{NUREG_1824}, simply because it is a simple, effective way to assess the fire's potential to damage critical equipment. It was also found that when comparing thousands of point to point measurements, the choice of metric did not change the final results.


\vspace{\parskip}
{\bf Lesson 6: Experiments are far from perfect.}

Because experimental uncertainty is often not reported, modelers have no choice but to compare their predictions to measurements without knowing the uncertainty of the reported test parameters or the measurements. The {\em experimental uncertainty} is a combination of the uncertainty in the measurement of the quantity of interest, like the gas temperature or heat flux, along with the propagated uncertainty of the measured test parameters, like the heat release rate or material properties.

In the NRC validation study~\cite{NUREG_1824}, Hamins estimated the experimental uncertainty for a variety of full-scale fire experiments. There were two uncertainty estimates needed for each quantity of interest. The first was an estimate, expressed in the form of a 95~\% confidence interval, of the uncertainty in the measurement of the quantity itself. For example, reported gas and surface temperatures were made with thermocouples of various designs (bare-bead, shielded, aspirated) with different size beads, metals, and so on. For each, one can estimate the
uncertainty in the reported measurement. Next, the uncertainty of the measurements of the reported test parameters was estimated, including the heat release rate, leakage area, ventilation rate, material properties, and so on. It was then necessary to calculate how the uncertainty in these parameters contributed to the uncertainty of the reported measurement. To do this, Hamins examined a number of empirical formulae that are widely used in fire protection engineering to determine the most important test parameters and their effect on the measured results. These formulae provided the means of propagating the parameter uncertainties through the experiment. For example, the correlation of McCaffrey, Quintiere and Harkleroad (MQH)~\cite{SFPE:Walton} asserts that the hot gas layer temperature rise, $T-T_0$, due to a compartment fire is proportional to the heat release rate, $\dot{Q}$, raised to the two-thirds power:
\begin{equation}
   T-T_0 = C \, \dot{Q}^{\frac{2}{3}}
\end{equation}
The constant, $C$, involves a number of geometric and thermo-physical parameters that are unique to the given fire scenario. By way of differentials, this empirical relationship can be expressed in the form:
\begin{equation}
   \frac{\Delta T}{T-T_0} \approx \frac{2}{3} \, \frac{\Delta \dot{Q}}{\dot{Q}}
\end{equation}
In words, the relative change in the temperature rise is approximately two-thirds the relative change in the heat release rate. Assuming that the numerical model exhibits the same functional relationship between the compartment temperature and the heat release rate, there is now a way to express the uncertainty of the model prediction as a function of the uncertainty of this most important input parameter. Often, the uncertainty of a measurement is expressed in the form of a 95~\% confidence interval, or two standard deviations or 2$\sigma$.
Thus, if the heat release rate of a fire is assumed with 95~\% confidence to be within 15~\% of the reported measurement, then the temperature predicted by the model has an uncertainty of at least 10~\%. Table~\ref{Parameter_Uncertainty} lists the most important physical parameters associated with various measured quantities in the experiments and their power dependence.

\begin{table}[ht]
\caption{Sensitivity of model outputs from Volume 2 of NUREG-1824~\cite{NUREG_1824}. }
\begin{center}
\begin{tabular}{|l|c|c|}
\hline
Output Quantity                                 & Input Parameter(s)    & Power Dependence \\ \hline \hline
HGL Temperature                                 & HRR                   & 2/3    \\ \hline
HGL Depth                                       & Door Height           & 1      \\ \hline
Gas Concentration                               & HRR                   & 1/2    \\ \hline
                                                & HRR                   & 1      \\ \cline{2-3}
\raisebox{1.5ex}[0pt]{Smoke Concentration}      & Soot Yield            & 1      \\ \hline
                                                & HRR                   & 2      \\ \cline{2-3}
Compartment Pressure                            & Leakage Rate          & 2      \\ \cline{2-3}
                                                & Ventilation Rate      & 2      \\ \hline
Heat Flux                                       & Heat Flux             & 4/3    \\ \hline
Surface Temperature                             & HRR                   & 2/3    \\ \hline
\end{tabular}
\end{center}
\label{Parameter_Uncertainty}
\end{table}

The uncertainty of the measured output quantity and the propagated uncertainty of each significant input parameter are combined via simple quadrature.  Hamins estimated the combined experimental uncertainties for ten quantities of interest in the NRC validation study. The results are summarized in Table~\ref{Uncertainty}, with each combined uncertainty reported in the form of a 95~\% confidence interval (i.e., $2 \, \widetilde{\sigma}_E$).
\begin{table}[ht]
\caption{Summary of Hamins' uncertainty estimates~\cite{NUREG_1824}. }
\begin{center}
\begin{tabular}{|l|c|}
\hline
Measured Quantity               & Combined Relative       \\
                                & Uncertainty, $2 \, \widetilde{\sigma}_E$       \\ \hline \hline
HGL Temperature                 & 0.14    \\ \hline
HGL Depth                       & 0.13    \\ \hline
Ceiling Jet Temperature         & 0.16    \\ \hline
Plume Temperature               & 0.14    \\ \hline
Gas Concentrations              & 0.09    \\ \hline
Smoke Concentration             & 0.33    \\ \hline
Pressure with Ventilation       & 0.80    \\ \hline
Pressure without Ventilation    & 0.40    \\ \hline
Heat Flux                       & 0.20    \\ \hline
Surface Temperature             & 0.14    \\ \hline
\end{tabular}
\end{center}
\label{Uncertainty}
\end{table}



\vspace{\parskip}
{\bf Lesson 7: The model uncertainty estimate need not be more complicated than the model itself.}

Perhaps the simplest expression of model uncertainty is to report that Model~X over or under-predicts Quantity~Y by Z~\%. In many instances this simple statement might be all that is required to demonstrate that a given model is appropriate for a given application. The relative uncertainty, Z, can be calculated simply by averaging the relative differences of the predicted and measured values. If there are an abundance of measurements from a variety of experiments, and these experiments are comparable in scope to the proposed application of the model (in the sense to be discussed below), then this simple quantification of model uncertainty may be sufficient to determine if the model is appropriate. However, with a bit more work, this simple quantification of model uncertainty can be expanded to include a measure of the scatter. It is not unusual, especially when there are many measurement points, that the {\em average} relative difference is quite small, often within a few percent. However, the scatter about the average can be considerable. In addition to the average relative difference, this scatter, expressed in the form of a standard deviation, can be used as uncertainty bounds for the predictions. This is particularly important to a regulatory agency like the NRC which routinely performs probabilistic risk assessments (PRA).

The method for calculating model uncertainty is described below. Further details on the method can be found in Ref.~\cite{McGrattan:Metrologia}. The goal of this analysis is to quantify the performance of the model in predicting a given quantity of interest (e.g., the hot gas layer temperature) with just two parameters; one that expresses the tendency for the model to under or over-predict the true value and one that expresses the degree of scatter about the true value.

Regardless of the exact form of the metric, what results from this exercise is a pair of numbers for each time history, $(E_i,M_i)$, where $i$ ranges from 1 to $n$ and both $M_i$ and $E_i$ are positive numbers expressing the increase in the value of a quantity above its ambient. As mentioned above, measurements from full-scale fire experiments often lack uncertainty estimates. In cases where the uncertainty is reported, it is usually expressed as either a standard deviation or confidence interval about the measured value. In other words, there is rarely a reported systematic bias in the measurement because if a bias can be quantified, the reported values are adjusted accordingly. For this reason, assume that a given experimental measurement, $E_i$, is normally distributed about the ``true'' value, $\theta_i$, and there is no systematic bias:
\begin{equation}
   E \; | \; \theta \sim N(\theta \; , \; \sigma_E^2) \label{expunc}
\end{equation}
The notation $E \; | \; \theta$ means that $E$ is conditional on a particular value of $\theta$. This is the usual way of defining a likelihood function. An important assumption in this analysis is that the model and experimental uncertainty can be expressed in {\em relative} terms. This assumption is based on the observation that the results of past validation exercises, when plotted as shown in Fig.~\ref{Normality}, form a wedge-shaped pattern that suggests that the difference between predicted and measured values is roughly proportional to the magnitude of the measured value. A common way in statistics of dealing with relative uncertainties is to work with the natural logarithm of the various quantities. To understand why, consider the so-called delta method in statistics~\cite{Oehlert:1992} where the distribution of a function of a normally distributed random variable $X \sim N(\mu,\sigma^2)$ is estimated:
\[g(X) \sim N \left( g(\mu) + g''(\mu) \, \sigma^2/2 \, , \, (g'(\mu) \, \sigma)^2\right)\]
The delta method provides an approximate distribution of the natural log of the random variable $E$:
\begin{equation}
   \ln E \; | \; \theta \sim N \left( \ln \theta - \frac{\widetilde{\sigma}_E^2}{2} \, , \,\widetilde{\sigma}_E^2 \right) \label{eeq}
\end{equation}
The purpose of applying the natural log to the random variable is so that its variance can be expressed in terms of the relative uncertainty, $\widetilde{\sigma}_E=\sigma_E/\theta$.

It cannot be assumed, as in the case of the experimental measurements, that the model predictions have no systematic bias. Instead, it is assumed that the model predictions are normally distributed about the true values multiplied by a bias factor, $\delta$:
\begin{equation}
   M \; | \; \theta \sim N \left(\delta \, \theta \, , \, \sigma_M^2 \right) \label{mdist}
\end{equation}
The standard deviation, $\sigma_M$, and the bias factor, $\delta$, characterize the model uncertainty. Again, the delta method renders a distribution for $\ln M$ whose parameters can be expressed in terms of a relative standard deviation:
\begin{equation}
   \ln M \; | \; \theta \sim N \left(\ln \delta +\ln \theta - \frac{\widetilde{\sigma}_M^2}{2} \; , \;
   \widetilde{\sigma}_M^2 \right) \quad ; \quad \widetilde{\sigma}_M=\frac{\sigma_M}{\delta \, \theta} \label{meq}
\end{equation}
Combining Eq.~(\ref{eeq}) with Eq.~(\ref{meq}) yields:
\begin{equation}
   \ln M  - \ln E = \ln(M/E) \sim N \left( \ln \delta - \frac{\widetilde{\sigma}_M^2}{2}+\frac{\widetilde{\sigma}_E^2}{2} \; ,
   \; \widetilde{\sigma}_M^2+\widetilde{\sigma}_E^2 \right) \label{lnMeq}
\end{equation}
Equation~(\ref{lnMeq}) is important because it does not involve the unknown $\theta$, but rather the known predicted and measured values of the quantity of interest plus an estimate of the experimental uncertainty. The mean of this distribution is estimated:
\begin{equation}
   \overline{\ln (M/E)} \approx \frac{1}{n} \, \sum_{i=1}^n \, \ln (M_i/E_i)
\end{equation}
The variance is estimated:
\begin{equation}
   \mathrm{Var} \Big( \ln (M/E) \Big) \approx \frac{1}{n-1} \sum_{i=1}^n \, \left[ \ln (M_i/E_i) - \overline{\ln (M/E)}  \right]^2 \label{stdev}
\end{equation}
from which the relative model standard deviation can be estimated:
\begin{equation}
   \widetilde{\sigma}_M \approx \sqrt{ \mathrm{Var}\Big( \ln (M/E) \Big) - \widetilde{\sigma}_E^2 } \label{sig_M}
\end{equation}
and the bias factor:
\begin{equation}
   \delta \approx \exp \left( \overline{\ln (M/E)} + \frac{ \widetilde{\sigma}_M^2}{2}-\frac{\widetilde{\sigma}_E^2}{2} \right) \label{delta}
\end{equation}
Equation~(\ref{sig_M}) imposes a constraint on the value of the experimental uncertainty, $\widetilde{\sigma}_E$. A further constraint is that $\widetilde{\sigma}_M$ cannot be less than $\widetilde{\sigma}_E$ because it is not possible to demonstrate that the model is more accurate than the measurements against which it is compared. Combining the two constraints leads to:
\begin{equation}
   \widetilde{\sigma}_E^2 < \frac{1}{2} \mathrm{Var}\Big( \ln (M/E) \Big)
\end{equation}
Equation~(\ref{mdist}) states that the model prediction, $M$, is normally distributed about the true value times the bias factor, $\delta \theta$. Under certain conditions~\cite{McGrattan:Metrologia}, it can be assumed that $\delta \theta$ is likewise distributed about $M$:
\begin{equation}
   \delta \theta \; | \; M \sim N \left(M \, , \, \sigma_M^2 \right) \label{thetaeq}
\end{equation}
Dividing by the bias factor, $\delta$, and applying the delta method once again yields:
\begin{equation}
   \theta \; | \; M \sim N \left( \frac{M}{\delta} \; , \; \widetilde{\sigma}_M^2 \left( \frac{M}{\delta} \right)^2 \right) \label{truth}
\end{equation}
In words, this expression states that the true value of a given quantity is normally distributed about the predicted value. The mean and standard deviation of the distribution are functions of the measured and predicted values of experiments that are comparable in scope to the scenario of interest. Before demonstrating how to use Eq.~(\ref{truth}), an aside. Until now, it has been assumed that all of the relevant uncertainties can be characterized by normal distributions. Equation~(\ref{lnMeq}) can be used to test this assumption, whereas Eqs.~(\ref{mdist}) and (\ref{expunc}) cannot. An examination of the results of the validation study discussed above~\cite{NUREG_1824} indicates that in cases where the number of pairs of $M$ and $E$ is greater than about 20, the values of $\ln (M/E)$ pass a common test for normality. Figure~\ref{Normality} contains a set of data from the validation study. There are 124 data points, for which $\ln (M/E)$ conforms quite well to a normal distribution. If normality cannot be assumed, alternative techniques for assessing model uncertainty with limited experimental data have been proposed, for example, see Ref.~\cite{Siu:1992}.
\begin{figure}[ht!]
\begin{tabular}{ll}
\includegraphics[width=2.8in]{../FDS_Validation_Guide/FIGURES/Wall_Temperature_Scatter} &
\includegraphics[width=2.8in]{../FDS_Validation_Guide/FIGURES/Wall_Temperature_Normality}
\end{tabular}
\caption[Testing the normality of validation data.]{The results of normality testing for a set of data taken from Reference~\cite{NUREG_1824}. On the
left is a comparison of measured vs predicted wall temperatures, and on the right is the distribution of $\ln(M/E)$. Note that the off-diagonal lines in the scatter plot
indicate the $2 \widetilde{\sigma}$ bounds for the experiments (long dash) and the model (short dash).}
\label{Normality}
\end{figure}

The results of the analysis described above are reported in the form of scatter plots similar to the one shown in Fig.~\ref{Normality}. It compares the measured and predicted wall surface temperatures for a particular model for several different sets of experiments. The standard deviations are reported in the form of 95~\% confidence intervals, or $2\widetilde{\sigma}$. The dashed off-diagonal lines depict these values graphically. The longer-dashed off-diagonal lines, which are centered about the solid diagonal line, indicate the experimental uncertainty.  The dash-dot line represents the model bias factor, in this case 1.13, indicating that the wall surface temperatures are 13~\% greater than the measurements, on average. The second set of off-diagonal lines, shown as short-dashed lines, indicates the model uncertainty. The information in Fig.~\ref{Normality} can be used as follows. Suppose this fire model is used to estimate the surface temperature within a compartment. The probability that the predicted temperature rise, $T_{\rm p}$, will exceed a critical temperature rise, $T_{\rm c}$ can be calculated using the {\em complimentary error function}:
\begin{equation}
   P(T>T_c) = \frac{1}{2} \hbox{erfc} \left( \frac{T_{\rm c} - \mu}{\sigma \sqrt{2}} \right) \quad ; \quad \mu=T_{\rm p}/\delta \quad ; \quad \sigma = \widetilde{\sigma}_M \, \frac{T_{\rm p}}{\delta}
\end{equation}
Note that this uncertainty estimate only considers the model uncertainty. It does not consider the uncertainty related to the input parameters. There are a variety of ways to treat parameter uncertainty listed in Refs.~\cite{ASTM:E1355,ISO16730,NUREG_1934,SFPE_G.06}




\vspace{\parskip}
{\bf Lesson 8: Model validation is not a blank check.}

The methodology discussed in the previous sections quantifies the model uncertainty, but there is an equally important consideration before using the model for a particular application. Standards like NFPA~805~\cite{NFPA_805} state that fire models shall only be applied within their applicable range. Again, this is a somewhat vague. For the original NRC/EPRI validation study~\cite{NUREG_1824}, twenty-six full-scale fire experiments from six different test series were used to evaluate the models' ability to estimate thirteen quantities of interest for fire scenarios that were judged to be typical of those that might occur in a nuclear power plant. Each series represented a typical fire scenario (for example, a fire in a switchgear room or turbine hall); however, the test parameters could not encompass every possible fire scenario that might occur within a plant. To better define the range of applicability of the validation study, the NRC and EPRI developed a list of various normalized parameters to characterize the scope of the experiments. These parameters express, for instance, the size of the fire relative to the size of the room, or the relative distance from the fire to critical equipment. The complete list of parameters are listed below.

The \underline{Fire Froude Number}, $\dot{Q}^*$, is a useful non-dimensional quantity for plume correlations and flame height estimates:
\begin{equation} 
   \dot{Q}^* = \frac{\dot{Q}}{\rho_\infty c_p T_\infty \sqrt{gD} D^2} 
\end{equation}
Here, $D$, is the equivalent diameter of the base of the fire, calculated $D=\sqrt{4A/\pi}$, where $A$ is the area of the base, $\dot{Q}$ is the peak heat release rate, $\rho_\infty$, $c_p$, $T_\infty$, and $g$ are the ambient gas density, specific heat, temperature and acceleration of gravity. The Fire Froude Number is essentially a expression of the fire's heat release rate divided by its base area. Jet fires are characterized by large Froude numbers. Brush fires are characterized by small Froude numbers. The Froude number is important for all types of models -- empirical correlations and zone models are not typically designed for Froude numbers that are not of order unity. CFD models may not be well-validated for large or small values.

The \underline{Flame to Ceiling Height Ratio}, $L_{\rm f}/H$, is a convenient way to express the physical size of the fire relative to the size of the room. The height of the visible flame, based on Heskestad's correlation, is estimated by:
\begin{equation} 
   L_{\rm f} = D \, \left( 3.7 \, (Q^*)^{2/5} - 1.02 \right) 
\end{equation}
This Flame to Ceiling Height Ratio is particularly significant for values approaching and exceeding 1, indicating flame impingement on the ceiling. As with the Froude number, empirical models and zone models are not designed for these kinds of fires, and CFD models may not be well-validated in this regime.

The \underline{Global Equivalence Ratio}, $\phi$, is the ratio of the mass flux of fuel to the mass flux of oxygen into the compartment, divided by the stoichiometric ratio:
\begin{equation} \phi = \frac{\dot{m}_f}{r\, \dot{m}{\hbox{\tiny O$_2$}}} \equiv  \frac{\dot{Q} \, \hbox{(kW)}}{13,100 \, \hbox{(kJ/kg)} \; \dot{m}_{\hbox{\tiny O$_2$}} } \quad ; \quad  \dot{m}_{\hbox{\tiny O$_2$}} = \left\{
   \begin{array}{r@{\quad:\quad}l}
      \frac{1}{2} \, 0.23 \, A_0 \sqrt{H_0} & \hbox{Natural Ventilation} \\[0.1in]
      0.23 \, \rho \, \dot{V}       & \hbox{Mechanical Ventilation} \end{array} \right.
\end{equation}
Here, $r$ is the stoichiometric ratio, $A_0$ is the area of the compartment opening, $H_0$ is the height of the opening, $\rho$ is the density of air, and $\dot{V}$ is the volume flow of air into the compartment. If $\phi<1$, the compartment is considered ``well-ventilated'' and if $\phi>1$, the compartment is considered ``under-ventilated.'' For values approaching and exceeding 1, empirical and zone models may be inappropriate unless they have some post-flashover functionality, and CFD models require special attention because some of their default input parameters or sub-models may be inappropriate.

The \underline{Compartment Aspect Ratios}, $W/H$ and $L/H$, indicate if the compartment is shaped like a hallway, typical room, or vertical shaft. This parameter is of particular importance for empirical correlations and zone models because both use similar sub-models that may not be applicable for narrow hallways or shafts. This parameter is less of a concern for CFD models unless the ratios are so large or small that the number of grid cells spanning any given dimension drops to less than about 10.

The \underline{Relative Distance along the Ceiling}, $r_{\rm cj}/H$, indicates the distance from the fire plume of a sprinkler, smoke detector, {\em etc.}, relative to the compartment height, $H$. This parameter is useful whenever the ceiling jet is an important component of the analysis. Empirical correlations and zone models should be checked to see if the ceiling jet correlation is appropriate close to or far from the plume centerline.

The \underline{Relative Distance from the Fire}, $r_{\rm rad}/D$, indicates whether a ``target'' is near or far from the fire. This parameter is useful when evaluating radiation calculation ranging from the simple point source method all the way through sophisticated solutions of the radiation transport equation.

When citing a set of experimental measurements as evidence that a particular model is valid, the end user ought to calculate the range of these six non-dimensional parameters and show that the intended application falls within this parameter space. This is just a formal way of stating that the validation experiments are comparable to or representative of the intended application. Sometimes the results are surprising. For example, a small fire in a small compartment is the modeling equivalent of a large fire in a large compartment. The non-dimensional parameters might be exactly the same. Disputes sometimes arise when a reviewing authority questions the validity of a model for an application that seems far different than the experiments against which the model is compared. This simple set of parameters will hopefully reduce these kinds of misunderstandings.


\vspace{\parskip}
{\bf CONCLUSION}

The NRC and EPRI are currently revising their fire model validation study~\cite{NUREG_1824} to include much more data than the original. Since its publication in 2007, the validation study has been widely referenced by those inside and outside of the nuclear industry even though in many instances the proposed model application falls well outside of the parameter space of the 26 experiments used in the study. Revision 1 of NUREG-1824 is to contain approximately 500 experiments and thousands of point to point comparisons. This will expand the range of applicability and also underpin the statistical description of the model uncertainty. 

\vspace{\parskip}
{\bf ACKNOWLEDGMENTS}

This work was supported by the U.S.~Nuclear Regulatory Commission, Office of Research. Special thanks to Mark Salley, David Stroup, and Jason Dreisbach. Thanks also to Blaza Toman of the NIST Statistical Engineering Division for her help in developing the model uncertainty methodology.

\vspace{\parskip}
{\bf REFERENCES}
\vspace{-0.5in}

\renewcommand{\refname}{}
\bibliography{../Bibliography/FDS_general}






\end{document}
